{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc168021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import math\n",
    "from typing import Dict, List, Union, Any, Optional, Set, Tuple\n",
    "import re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===== CONFIG =====\n",
    "MAGI_DB_PATH = \"/projects/klybarge/pcori_ad/magi/magi_db/magi.db\"  # read-only\n",
    "OUT_DIR = \"./MAGI_LASSO\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "EDGE_TABLE = \"magi_counts_top500\"   \n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "\n",
    "TOP_K = 500  # 500 for others\n",
    "\n",
    "TARGETS = [\n",
    "    \"aa_meas_citalopram_rem\",\n",
    "]\n",
    "\n",
    "############### MAGI FUNCTION ###############\n",
    "\"\"\"\n",
    "MAGI: Dependent Bayes with Temporal Ordering — Reference Implementation\n",
    "-------------------------------------------------------------------------------\n",
    "This file adds **step-by-step comments** to the function `analyze_causal_sequence_py`. The comments mirror the proposed\n",
    "algorithm sections:\n",
    "\n",
    "1) Determination of Temporal Order\n",
    "2) Estimation of Dependent Bayes (T-values, λ-links, and D-values)\n",
    "3) Logistic link to produce P(Y=1 | Z)\n",
    "\n",
    "INPUT TABLE EXPECTATIONS (long format, one row per directed pair):\n",
    "- target_concept_code : str (# left node in the edge; in many places this\n",
    "denotes the earlier event)\n",
    "- concept_code : str (# right node in the edge; the later event)\n",
    "- n_code_target : float/int (# co-occurrence count for concept & target\n",
    "in the *target=Y* stratum; used as 'a')\n",
    "- n_code_no_target : float/int (# co-occurrence count for concept & target\n",
    "in the *target=¬Y* stratum; used as 'b')\n",
    "- n_target : float/int (# total Y count for this target concept, per edge row; we take max within a block)\n",
    "- n_no_target : float/int (# total ¬Y count; max within a block)\n",
    "- no_code_no_target : float/int (# optional, computed if missing as n_no_target - n_code_no_target; clipped ≥0)\n",
    "- n_target_before_code: float/int (# count where target occurred before code)\n",
    "- n_code_before_target: float/int (# count where code occurred before target)\n",
    "\n",
    "NOTES ON TEMPORAL COUNTS:\n",
    "For a row with target_concept_code = A and concept_code = B, the columns\n",
    "`n_code_before_target` and `n_target_before_code` are interpreted as:\n",
    "- n_code_before_target: # of persons where **B happened before A**\n",
    "- n_target_before_code: # of persons where **A happened before B**\n",
    "We aggregate these across j≠i to compute *temporal scores* for each event.\n",
    "\n",
    "PIECEWISE, SAMPLE-SIZE–ANCHORED ADJUSTMENTS:\n",
    "- For odds terms that would be 0 or ∞ due to zero cells, we replace the\n",
    "offending odds with 1/(N+1) or (N+1)/1, where N is the size of the\n",
    "appropriate stratum, so all ratios remain finite and interpretable.\n",
    "\n",
    "RETURN VALUE:\n",
    "A dict with temporal ordering, T-values, λ-vectors, D-values, coefficients\n",
    "for a logistic link, a `predict_proba` callable, and trace tables.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_causal_sequence_py(\n",
    "    data: Union[str, pd.DataFrame],\n",
    "    name_map: Dict[str, str],\n",
    "    events: List[str],\n",
    "    force_outcome=None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Compute temporal order, dependent-Bayes direct effects (D), and\n",
    "    a logistic-link probability for outcome Y from pairwise counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str | DataFrame\n",
    "        CSV path or in-memory DataFrame with the columns described above.\n",
    "    name_map : Dict[str, str]\n",
    "        Optional mapping raw code -> friendly label. If provided, both\n",
    "        `target_concept_code` and `concept_code` are replaced.\n",
    "    events : List[str]\n",
    "        List of event names/codes to restrict the analysis to. If `None`,\n",
    "        events are auto-detected from the table and intersected.\n",
    "    force_outcome : str | None\n",
    "        If provided and found among events, this event is forced to be the\n",
    "        **final** node (i.e., the outcome) in the temporal order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        - sorted_scores : pd.Series of temporal scores (desc)\n",
    "        - temporal_order: list of events (outcome at the end)\n",
    "        - order_used    : same as temporal_order\n",
    "        - T_val         : pd.Series of total effects T_{k,Y}\n",
    "        - D_val         : pd.Series of direct effects D_{k,Y}\n",
    "        - coef_df       : pd.DataFrame of coefficients (β_k and intercept)\n",
    "        - lambda_l      : dict[str -> pd.Series] of λ_{k,j} vectors\n",
    "        - trace_df      : pd.DataFrame detailing the backward recursion steps\n",
    "        - invalid_predictors: list of predictors whose log(D) was invalid\n",
    "        - beta_0, beta, logit_predictors, predict_proba: logistic elements\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 0) INGEST & BASIC VALIDATION\n",
    "    # ---------------------------------------------------------------------\n",
    "    if isinstance(data, str):\n",
    "        # Read from CSV path\n",
    "        df = pd.read_csv(data)\n",
    "    else:\n",
    "        # Work on a copy to avoid mutating caller's object\n",
    "        df = data.copy()\n",
    "\n",
    "    # Ensure required identifier columns are present\n",
    "    for col in [\"target_concept_code\", \"concept_code\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Optional recoding to human-friendly labels\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "        df[\"concept_code\"] = df[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    # Numeric columns the algorithm expects\n",
    "    need = [\n",
    "        \"n_code_target\", \"n_code_no_target\", \"n_target\", \"n_no_target\",\n",
    "        \"n_target_before_code\", \"n_code_before_target\",\n",
    "    ]\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    # If precomputed total_effect exists (for λ or sanity checks), coerce to numeric\n",
    "    has_total = \"total_effect\" in df.columns\n",
    "    if has_total:\n",
    "        df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) EVENT SET (optional auto-detect) & TYPE COERCION\n",
    "    # ---------------------------------------------------------------------\n",
    "    if events is None:\n",
    "        # Auto-detect: intersect events appearing on both sides of edges\n",
    "        ev_targets = df[\"target_concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        ev_code = df[\"concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        events = sorted(set(ev_targets).intersection(ev_code))\n",
    "        if len(events) == 0:\n",
    "            # Fall back to union if intersection is empty\n",
    "            events = sorted(set(ev_targets) | set(ev_code))\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events after auto-detection.\")\n",
    "\n",
    "    # Keep only rows whose endpoints are both in the chosen event set\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "\n",
    "    # Coerce numeric columns robustly (invalid -> NaN); subsequent ops handle NaNs\n",
    "    for c in need:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # If `no_code_no_target` missing, derive as (n_no_target - n_code_no_target) ≥ 0\n",
    "    if \"no_code_no_target\" not in df.columns:\n",
    "        df[\"no_code_no_target\"] = (df[\"n_no_target\"] - df[\"n_code_no_target\"]).clip(lower=0)\n",
    "    else:\n",
    "        df[\"no_code_no_target\"] = pd.to_numeric(df[\"no_code_no_target\"], errors=\"coerce\").clip(lower=0)\n",
    "\n",
    "    # Helper: total count for an event (max n_target where that event is target)\n",
    "    # This mirrors your original choice; change to .sum() if warranted.\n",
    "    def C_of(ev: str) -> float:\n",
    "        sub = df[df[\"target_concept_code\"] == ev]\n",
    "        if sub.empty:\n",
    "            return float(\"nan\")\n",
    "        C = pd.to_numeric(sub[\"n_target\"], errors=\"coerce\").max()\n",
    "        return float(C) if pd.notna(C) and np.isfinite(C) else float(\"nan\")\n",
    "\n",
    "    # Helper: numerically safe log (log(1)=0 for invalid/≤0)\n",
    "    def safe_log(x: float) -> float:\n",
    "        try:\n",
    "            xv = float(x)\n",
    "        except (TypeError, ValueError):\n",
    "            return 0.0\n",
    "        if not np.isfinite(xv) or xv <= 0.0:\n",
    "            return 0.0\n",
    "        return math.log(xv)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) TEMPORAL ORDER — pairwise-before counts → per-node score\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Score(Z_i) = Σ_{j≠i} [ C(Z_i≪Z_j) - C(Z_j≪Z_i) + C(Z_i ∩ ¬Z_j) - C(Z_j ∩ ¬Z_i) ]\n",
    "    # Here we implement the *before/after* portion using the provided columns.\n",
    "    # If only presence/absence terms are available, you may approximate using\n",
    "    # the last two terms.\n",
    "    scores: Dict[str, float] = {}\n",
    "    for zk in events:\n",
    "        s = 0.0\n",
    "        for zj in [x for x in events if x != zk]:\n",
    "            # For the pair (zj, zk), we interpret:\n",
    "            #   n_code_before_target   — # where zk (code) before zj (target)\n",
    "            #   n_target_before_code   — # where zj (target) before zk (code)\n",
    "            rowr = df[(df[\"target_concept_code\"] == zj) & (df[\"concept_code\"] == zk)]\n",
    "            if not rowr.empty:\n",
    "                s += float(rowr[\"n_code_before_target\"].sum(skipna=True) -\n",
    "                           rowr[\"n_target_before_code\"].sum(skipna=True))\n",
    "        scores[zk] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Outcome selection:\n",
    "    #  - If `force_outcome` is provided and present, put it at the end.\n",
    "    #  - Else, default to the top-scoring node as outcome.\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome_event = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "    else:\n",
    "        outcome_event = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "\n",
    "    # Propagation order is the temporal order; last is outcome Y\n",
    "    events_order = temporal_order\n",
    "    outcome = events_order[-1]\n",
    "    antecedents = events_order[:-1]\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3) T-VALUES (TOTAL EFFECTS) & λ-LINKS BETWEEN ANTECEDENTS\n",
    "    # ---------------------------------------------------------------------\n",
    "    # For each antecedent k, we compute T_{k,Y} as an odds ratio between\n",
    "    # strata Z_k=1 and Z_k=0 with sample-size–anchored fixes for zero cells.\n",
    "    # For λ_{k,j} (dependence of j given k), we first use precomputed\n",
    "    # `total_effect` if present.\n",
    "    # If λ is missing (no recorded dependence), assume independence\n",
    "    # and treat the conditional contribution as 0 in the adjustment sum.\n",
    "\n",
    "    T_val = pd.Series(0.0, index=antecedents, dtype=float)  # T_{k,Y}\n",
    "    D_val = pd.Series(np.nan, index=antecedents, dtype=float)  # D_{k,Y}\n",
    "    lambda_l: Dict[str, pd.Series] = {}  # per-k vector of λ_{k,j} for j after k and before Y\n",
    "\n",
    "    for k in antecedents:\n",
    "        # ---- Contingency for (k, outcome) ----\n",
    "        row_ko = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == outcome)]\n",
    "\n",
    "        # a: Y∩k    b: ¬Y∩k    c: Y∩¬k    d: ¬Y∩¬k\n",
    "        a = float(row_ko[\"n_code_target\"].sum(skipna=True))            # co-occurrence in Y\n",
    "        b = float(row_ko[\"n_code_no_target\"].sum(skipna=True))         # co-occurrence in ¬Y\n",
    "\n",
    "        # If n_target_no_code absent, approximate c by (max n_target - a)\n",
    "        if \"n_target_no_code\" in row_ko.columns:\n",
    "            c = float(row_ko[\"n_target_no_code\"].sum(skipna=True))\n",
    "        else:\n",
    "            c = float(pd.to_numeric(row_ko[\"n_target\"], errors=\"coerce\").max() - a)\n",
    "\n",
    "        # If no_code_no_target absent, approximate d by (max n_no_target - b)\n",
    "        if \"no_code_no_target\" in row_ko.columns:\n",
    "            d = float(row_ko[\"no_code_no_target\"].sum(skipna=True))\n",
    "        else:\n",
    "            d = float(pd.to_numeric(row_ko[\"n_no_target\"], errors=\"coerce\").max() - b)\n",
    "\n",
    "        N1, N0 = a + b, c + d  # stratum sizes for Z_k=1 and Z_k=0\n",
    "\n",
    "        # ---- Sample-size–anchored odds for Z_k=1 ----\n",
    "        if a == 0:\n",
    "            odds_pos_adj = 1.0 / (N1 + 1.0)  # 0 → tiny positive odds\n",
    "        elif b == 0:\n",
    "            odds_pos_adj = (N1 + 1.0) / 1.0  # ∞ → capped by (N1+1)\n",
    "        else:\n",
    "            odds_pos_adj = a / b\n",
    "\n",
    "        # ---- Sample-size–anchored odds for Z_k=0 ----\n",
    "        if c == 0:\n",
    "            odds_neg_adj = 1.0 / (N0 + 1.0)\n",
    "        elif d == 0:\n",
    "            odds_neg_adj = (N0 + 1.0) / 1.0\n",
    "        else:\n",
    "            odds_neg_adj = c / d\n",
    "\n",
    "        # Total effect T_{k,Y}\n",
    "        T_val.loc[k] = float(odds_pos_adj / odds_neg_adj)\n",
    "\n",
    "        # ---- λ_{k,j}: dependence of j on k for nodes j after k (and before Y) ----\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = []\n",
    "        for j in js:\n",
    "            row_kj = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == j)]\n",
    "            if row_kj.empty:\n",
    "                lam_pairs.append((j, 0.0))  # no evidence of dependence\n",
    "                continue\n",
    "\n",
    "            # Otherwise approximate λ with piecewise, size-anchored logic\n",
    "            # C11 = C(j∩k); Cj_not_k = C(j∩¬k); Ck = C(k)\n",
    "            C11 = float(row_kj[\"n_code_target\"].sum(skipna=True))  # re-using the same column name for co-occurrence\n",
    "            if \"n_code_no_target\" in row_kj.columns:\n",
    "                Cj_not_k = float(row_kj[\"n_code_no_target\"].sum(skipna=True))\n",
    "            else:\n",
    "                Cj = C_of(j)\n",
    "                Cj_not_k = 0.0 if (not np.isfinite(Cj)) else max(Cj - C11, 0.0)\n",
    "            Ck = C_of(k)\n",
    "\n",
    "            if Cj_not_k == 0:\n",
    "                L = 1.0 + C11          # always-with-k → boost\n",
    "            elif C11 == 0:\n",
    "                L = 1.0 / (1.0 + Cj_not_k)  # never-with-k → downweight\n",
    "            elif np.isfinite(Ck) and Ck > 0:\n",
    "                L = C11 / Ck           # normalized co-occurrence\n",
    "            else:\n",
    "                L = 0.0\n",
    "\n",
    "            lam_pairs.append((j, float(L)))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_pairs}, dtype=float)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4) BACKWARD RECURSION — resolve D_{k,Y} from T and λ\n",
    "    # ---------------------------------------------------------------------\n",
    "    # D_{k,Y} = ( T_{k,Y} - Σ_i λ_{k,k+i} * D_{k+i,Y} ) / ( 1 - Σ_i λ_{k,k+i} )\n",
    "    # Start at the last antecedent (just before Y): D := T, since there are no\n",
    "    # downstream nodes to adjust for.\n",
    "\n",
    "    trace_rows = []  # for human-auditable tracing of the recursion\n",
    "\n",
    "    last_anc = antecedents[-1] if antecedents else None\n",
    "    if last_anc is not None:\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "        trace_rows.append({\n",
    "            \"stage\": \"Last 2 Nodes\",\n",
    "            \"nodes\": f\"{last_anc} - {outcome}\",\n",
    "            \"k\": last_anc,\n",
    "            \"T_kY\": T_val.loc[last_anc],\n",
    "            \"lambda_terms\": None,\n",
    "            \"sum_lambda\": 0.0,\n",
    "            \"D_kY\": D_val.loc[last_anc],\n",
    "            \"log_D\": safe_log(D_val.loc[last_anc]),\n",
    "        })\n",
    "\n",
    "    if len(antecedents) > 1:\n",
    "        # Walk backwards through the remaining antecedents\n",
    "        for k in list(reversed(antecedents))[1:]:\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            # When computing the adjustment, use **only** D-values for nodes that\n",
    "            # are after k and already resolved (present in lam_vec index).\n",
    "            code = list(lam_vec.index)\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vec.reindex(code).values * D_val.reindex(code).values))\n",
    "            den = 1.0 - float(np.nansum(lam_vec.values))  # may approach 0 if λ's are large\n",
    "\n",
    "            # If den is pathological (≤0 or NaN), fall back to T (neutralization).\n",
    "            D_val.loc[k] = (num / den) if np.isfinite(num / den) else T_val.loc[k]\n",
    "\n",
    "            span = len(events_order) - events_order.index(k) + 1\n",
    "            lam_str = \", \".join(\n",
    "                f\"λ_{events_order.index(k)+1}{events_order.index(c)+1}={lam_vec[c]:.6f}\"\n",
    "                for c in code\n",
    "            ) if len(lam_vec) else None\n",
    "\n",
    "            trace_rows.append({\n",
    "                \"stage\": f\"Last {span} Nodes\",\n",
    "                \"nodes\": \" - \".join([k] + events_order[events_order.index(k)+1:]),\n",
    "                \"k\": k,\n",
    "                \"T_kY\": T_val.loc[k],\n",
    "                \"lambda_terms\": lam_str,\n",
    "                \"sum_lambda\": float(np.nansum(lam_vec.values)),\n",
    "                \"D_kY\": D_val.loc[k],\n",
    "                \"log_D\": safe_log(D_val.loc[k]),\n",
    "            })\n",
    "\n",
    "    trace_df = pd.DataFrame(trace_rows)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 5) COEFFICIENTS — map D's onto a logistic link\n",
    "    # ---------------------------------------------------------------------\n",
    "    # We model:  logit P(Y=1 | Z) = β0 + Σ_k β_k * Z_k,  with β_k = log D_{k,Y}\n",
    "    # Intercept β0 is set by the marginal prevalence of Y (from `n_target` &\n",
    "    # `n_no_target`) for the outcome rows.\n",
    "\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    if resp_rows.empty:\n",
    "        raise ValueError(f\"No rows for outcome '{outcome}'.\")\n",
    "\n",
    "    n_t = resp_rows[\"n_target\"].dropna().iloc[0] if resp_rows[\"n_target\"].dropna().size else np.nan\n",
    "    n_n = resp_rows[\"n_no_target\"].dropna().iloc[0] if resp_rows[\"n_no_target\"].dropna().size else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))  # stable enough for p∈(0,1)\n",
    "\n",
    "    # β_k = log(D_{k,Y}); protect against non-positive D by mapping to 0\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    D_pos = D_clean.where(D_clean > 0)\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        beta_vals = np.log(D_pos.to_numpy())\n",
    "    beta_k_raw = pd.Series(beta_vals, index=D_val.index)\n",
    "    invalid_predictors = list(beta_k_raw[~np.isfinite(beta_k_raw)].index)\n",
    "\n",
    "    beta_k = beta_k_raw.copy()\n",
    "    beta_k[~np.isfinite(beta_k)] = 0.0  # neutralize invalid predictors\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_k.index) + [\"(intercept)\"],\n",
    "        \"beta\": list(beta_k.astype(float).values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 6) PREDICT_PROBA — vectorized logistic link\n",
    "    # ---------------------------------------------------------------------\n",
    "    predictors = list(beta_k.index)\n",
    "    beta_vec = beta_k.astype(float).values\n",
    "\n",
    "    def predict_proba(z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]) -> Union[float, np.ndarray, pd.Series]:\n",
    "        \"\"\"Compute P(Y=1 | Z) using the logistic link.\n",
    "\n",
    "        Accepts:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame with columns containing any/all of `predictors` (others ignored)\n",
    "\n",
    "        Returns:\n",
    "          - float for 1D inputs; np.ndarray or pd.Series for vectorized inputs\n",
    "        \"\"\"\n",
    "        if isinstance(z, pd.DataFrame):\n",
    "            Z = z.reindex(columns=predictors, fill_value=0).astype(float).to_numpy()\n",
    "            eta = beta_0 + Z @ beta_vec\n",
    "            # Stable sigmoid via clipping; avoids overflow for extreme η\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "        if isinstance(z, (dict, pd.Series)):\n",
    "            v = np.array([float(z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            eta = beta_0 + float(v @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "\n",
    "        arr = np.asarray(z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            eta = beta_0 + float(arr @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "        else:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*, {len(predictors)}), got {arr.shape}\")\n",
    "            eta = beta_0 + arr @ beta_vec\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7) PACKAGE RESULTS\n",
    "    # ---------------------------------------------------------------------\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"trace_df\": trace_df,\n",
    "        \"invalid_predictors\": invalid_predictors,\n",
    "        # Logistic link outputs:\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n",
    "\n",
    "# ===== SNOMED descendants support (all descendants)=====\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "IS_A = \"116680003\"\n",
    "CHAR_TYPES = {\n",
    "    \"inferred\": \"900000000000011006\",  # classification hierarchy\n",
    "    \"stated\":   \"900000000000010007\",\n",
    "}\n",
    "\n",
    "SNOMED_REL_FULL_US = \"/projects/klybarge/pcori_ad/magi/Test/Test/sct2_Relationship_Full_US1000124_20250901.txt\"\n",
    "\n",
    "def build_is_a_snapshot(full_rel_path: str, characteristic: str = \"inferred\") -> pd.DataFrame:\n",
    "    \"\"\"From a Full RF2 file, return the *current* active IS-A rows.\"\"\"\n",
    "    use_cols = [\n",
    "        \"id\",\"effectiveTime\",\"active\",\"moduleId\",\n",
    "        \"sourceId\",\"destinationId\",\"relationshipGroup\",\n",
    "        \"typeId\",\"characteristicTypeId\",\"modifierId\"\n",
    "    ]\n",
    "    df = pd.read_csv(full_rel_path, sep=\"\\t\", dtype=str, usecols=use_cols)\n",
    "    df = df[(df[\"typeId\"] == IS_A) & (df[\"characteristicTypeId\"] == CHAR_TYPES[characteristic])]\n",
    "    df[\"effectiveTime_num\"] = df[\"effectiveTime\"].astype(int)\n",
    "    idx = df.groupby(\"id\")[\"effectiveTime_num\"].idxmax()\n",
    "    snap = df.loc[idx]\n",
    "    snap = snap[snap[\"active\"] == \"1\"][[\"sourceId\",\"destinationId\"]].reset_index(drop=True)\n",
    "    return snap\n",
    "\n",
    "# Build parent→children map once (lazy init so the script still runs if file missing)\n",
    "__SNAP_REL__ = None\n",
    "__P2C__ = None\n",
    "def _ensure_graph():\n",
    "    global __SNAP_REL__, __P2C__\n",
    "    if __SNAP_REL__ is None:\n",
    "        __SNAP_REL__ = build_is_a_snapshot(SNOMED_REL_FULL_US, characteristic=\"inferred\")\n",
    "    if __P2C__ is None:\n",
    "        __P2C__ = defaultdict(set)\n",
    "        for parent, child in zip(__SNAP_REL__[\"destinationId\"], __SNAP_REL__[\"sourceId\"]):\n",
    "            __P2C__[parent].add(child)\n",
    "\n",
    "def find_descendants_sct(concept_id: str) -> set:\n",
    "    \"\"\"All is-a descendants (children, grandchildren, ...) for a SNOMED conceptId.\"\"\"\n",
    "    _ensure_graph()\n",
    "    out = set()\n",
    "    q = deque([concept_id])\n",
    "    while q:\n",
    "        cur = q.popleft()\n",
    "        for kid in __P2C__.get(cur, ()):\n",
    "            if kid not in out:\n",
    "                out.add(kid)\n",
    "                q.append(kid)\n",
    "    return out\n",
    "\n",
    "def extract_snomed_id(code: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    If code looks like 'dx_SNOMED_<digits>' return '<digits>', else None.\n",
    "    \"\"\"\n",
    "    m = re.fullmatch(r\"dx_SNOMED_(\\d+)\", str(code))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def snomed_aliases_for_outcome(outcome_code: str) -> Tuple[Optional[Set[str]], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Given an outcome like 'dx_SNOMED_254645002', return:\n",
    "      - aliases_codes: {'dx_SNOMED_<id>', ...} including the root itself (or None if not SNOMED)\n",
    "      - name_map: mapping every alias_code -> outcome_code (canonical)\n",
    "    \"\"\"\n",
    "    root_id = extract_snomed_id(outcome_code)\n",
    "    if root_id is None:\n",
    "        return None, {}\n",
    "\n",
    "    desc = find_descendants_sct(root_id)\n",
    "    all_ids = {root_id} | set(desc)\n",
    "    aliases_codes: Set[str] = {f\"dx_SNOMED_{sid}\" for sid in all_ids}\n",
    "    name_map: Dict[str, str] = {alias: outcome_code for alias in aliases_codes if alias != outcome_code}\n",
    "    return aliases_codes, name_map\n",
    "\n",
    "# ========= helpers =========\n",
    "\n",
    "def _ensure_derived_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure derived counts (n_target_no_code, no_code_no_target) exist,\n",
    "    coerce numeric columns safely, and (if absent) compute per-edge total_effect\n",
    "    using sample-size–anchored odds. Returns a copy.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # ---- 1) Base numeric columns ----\n",
    "    base = [\"n_code_target\", \"n_code_no_target\", \"n_target\", \"n_no_target\"]\n",
    "    for c in base:\n",
    "        if c not in out.columns:\n",
    "            out[c] = 0\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).clip(lower=0).astype(float)\n",
    "\n",
    "    # ---- 2) Complements (clip to ≥ 0) ----\n",
    "    # Y & (k=0)\n",
    "    if \"n_target_no_code\" not in out.columns:\n",
    "        out[\"n_target_no_code\"] = (out[\"n_target\"] - out[\"n_code_target\"]).clip(lower=0)\n",
    "    else:\n",
    "        out[\"n_target_no_code\"] = pd.to_numeric(out[\"n_target_no_code\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "\n",
    "    # (k=0) & (Y=0)\n",
    "    if \"no_code_no_target\" not in out.columns:\n",
    "        out[\"no_code_no_target\"] = (out[\"n_no_target\"] - out[\"n_code_no_target\"]).clip(lower=0)\n",
    "    else:\n",
    "        out[\"no_code_no_target\"] = pd.to_numeric(out[\"no_code_no_target\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "\n",
    "    # ---- 3) total_effect (odds_pos / odds_neg) if absent ----\n",
    "    if \"total_effect\" not in out.columns:\n",
    "        a  = out[\"n_code_target\"].astype(float)        # k=1, Y=1\n",
    "        b  = out[\"n_code_no_target\"].astype(float)     # k=1, Y=0\n",
    "        c  = out[\"n_target_no_code\"].astype(float)     # k=0, Y=1\n",
    "        d  = out[\"no_code_no_target\"].astype(float)    # k=0, Y=0\n",
    "\n",
    "        N1 = a + b\n",
    "        N0 = c + d\n",
    "\n",
    "        # Sample-size–anchored odds with neutral fallback when stratum size is zero\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            odds_pos = np.where(\n",
    "                N1 == 0, 1.0,                              # no data in Z_k=1 → neutral\n",
    "                np.where(\n",
    "                    (a > 0) & (b > 0), a / b,\n",
    "                    np.where(b == 0, N1 + 1.0, 1.0 / (N1 + 1.0))\n",
    "                )\n",
    "            )\n",
    "            odds_neg = np.where(\n",
    "                N0 == 0, 1.0,                              # no data in Z_k=0 → neutral\n",
    "                np.where(\n",
    "                    (c > 0) & (d > 0), c / d,\n",
    "                    np.where(d == 0, N0 + 1.0, 1.0 / (N0 + 1.0))\n",
    "                )\n",
    "            )\n",
    "\n",
    "            te = odds_pos / odds_neg\n",
    "\n",
    "        # Clean any residual non-finite values to neutral 1.0\n",
    "        te = np.where(np.isfinite(te), te, 1.0)\n",
    "        out[\"total_effect\"] = te.astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "# TOPK_HI/LO\n",
    "def _select_top_by_te_unique_k(k_to_T: pd.DataFrame, top_k: int = TOP_K,\n",
    "                                   hi: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balanced selection without fallback:\n",
    "      - Take up to top_k//2 strongest risk (TE >= hi)\n",
    "      - Take up to top_k - top_k//2 strongest protective (TE <= 1/hi)\n",
    "      - If either side is short, do NOT fill from elsewhere; total may be < top_k\n",
    "      - Return one row per k (target_concept_code), ranked by extremeness\n",
    "    \"\"\"\n",
    "    df = k_to_T.copy()\n",
    "\n",
    "    # Ensure total_effect numeric\n",
    "    df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effect\"]).copy()\n",
    "    if df.empty:\n",
    "        print(\"[SELECT] no rows with total_effect; returning empty selection.\")\n",
    "        return df\n",
    "\n",
    "    # Extremeness symmetrical around 1\n",
    "    df[\"effect_strength\"] = np.where(df[\"total_effect\"] >= 1.0,\n",
    "                                     df[\"total_effect\"],\n",
    "                                     1.0 / df[\"total_effect\"])\n",
    "\n",
    "    # One row per k (keep most extreme)\n",
    "    best_per_k = (df.sort_values(\"effect_strength\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"target_concept_code\"], keep=\"first\"))\n",
    "\n",
    "    HI = hi\n",
    "    LO = 1.0 / hi\n",
    "\n",
    "    risk_pool = best_per_k[best_per_k[\"total_effect\"] >= HI].copy()\n",
    "    prot_pool = best_per_k[best_per_k[\"total_effect\"] <= LO].copy()\n",
    "\n",
    "    # Target split\n",
    "    want_risk = top_k // 2\n",
    "    want_prot = top_k - want_risk\n",
    "\n",
    "    # Take strongest from each side, capped by availability\n",
    "    take_risk = min(len(risk_pool), want_risk)\n",
    "    take_prot = min(len(prot_pool), want_prot)\n",
    "\n",
    "    sel_risk = risk_pool.nlargest(take_risk, \"effect_strength\")\n",
    "    sel_prot = prot_pool.nlargest(take_prot, \"effect_strength\")\n",
    "\n",
    "    selected = pd.concat([sel_risk, sel_prot], ignore_index=True)\n",
    "\n",
    "    print(f\"[SELECT] total unique k={len(best_per_k):,}  \"\n",
    "          f\"risk={len(risk_pool):,}  prot={len(prot_pool):,}  \"\n",
    "          f\"selected(total)={selected['target_concept_code'].nunique()}  \"\n",
    "          f\"with: risk={take_risk}, prot={take_prot}\")\n",
    "\n",
    "    return selected.reset_index(drop=True)\n",
    "\n",
    "def _fetch_k_to_T(conn, target_code: str) -> pd.DataFrame:\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- k (predictor)\n",
    "             ccn.concept_code AS concept_code           -- T (outcome)\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE ccn.concept_code = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[target_code])\n",
    "\n",
    "def _fetch_k_to_T_in(conn, target_codes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch only k→T rows where concept_code IN (target_codes).\n",
    "    Uses a temp table to avoid SQLite's variable limit.\n",
    "    \"\"\"\n",
    "    tmp_name = \"tmp_targets_magi\"\n",
    "    with conn:\n",
    "        conn.execute(f\"DROP TABLE IF EXISTS {tmp_name}\")\n",
    "        conn.execute(f\"CREATE TEMP TABLE {tmp_name}(concept_code TEXT)\")\n",
    "        conn.executemany(f\"INSERT INTO {tmp_name}(concept_code) VALUES (?)\", [(c,) for c in target_codes])\n",
    "\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- k (predictor)\n",
    "             ccn.concept_code AS concept_code           -- T (outcome)\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      JOIN {tmp_name} tmp     ON ccn.concept_code         = tmp.concept_code\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn)\n",
    "\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list):\n",
    "    ph = \",\".join([\"?\"] * len(events_list))\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,\n",
    "             ccn.concept_code AS concept_code\n",
    "      FROM {EDGE_TABLE} m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=list(events_list))\n",
    "\n",
    "import pandas as pd\n",
    "import os, time, re\n",
    "from collections import defaultdict\n",
    "\n",
    "def expand_targets_with_descendants(targets: List[str]) -> Dict[str, Set[str]]:\n",
    "    out: Dict[str, Set[str]] = {}\n",
    "    for T in targets:\n",
    "        aliases, _name_map = snomed_aliases_for_outcome(T)\n",
    "        out[T] = aliases if aliases is not None else {T}\n",
    "    return out\n",
    "\n",
    "def print_full_targets(targets: List[str], *, save_csv: bool = True, out_dir: str = \"./\"):\n",
    "    \"\"\"\n",
    "    Pretty-print the expanded target definitions and (optionally) save a CSV with all rows:\n",
    "      root_target, alias_code, alias_conceptId, is_root\n",
    "    \"\"\"\n",
    "    expanded = expand_targets_with_descendants(targets)\n",
    "\n",
    "    # pretty print to console\n",
    "    for T, alias_set in expanded.items():\n",
    "        # count and show a short sample\n",
    "        n = len(alias_set)\n",
    "        sample = \", \".join(sorted(list(alias_set))[:10])\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"[TARGET] {T}\")\n",
    "        print(f\"  aliases (including descendants): {n}\")\n",
    "        print(f\"  sample: {sample}{' ...' if n > 10 else ''}\")\n",
    "\n",
    "    # optional CSV dump\n",
    "    if save_csv:\n",
    "        rows = []\n",
    "        for T, alias_set in expanded.items():\n",
    "            root_id = extract_snomed_id(T)\n",
    "            for alias in sorted(alias_set):\n",
    "                alias_id = extract_snomed_id(alias)\n",
    "                rows.append({\n",
    "                    \"root_target\": T,\n",
    "                    \"alias_code\": alias,\n",
    "                    \"alias_conceptId\": alias_id if alias_id else \"\",\n",
    "                    \"is_root\": (alias == T)\n",
    "                })\n",
    "        df = pd.DataFrame(rows, columns=[\"root_target\",\"alias_code\",\"alias_conceptId\",\"is_root\"])\n",
    "        ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        path = os.path.join(out_dir, f\"targets_expanded_{ts}.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"\\n[SAVED] Full target expansions → {path}\")\n",
    "\n",
    "    return expanded\n",
    "\n",
    "# ---- call (run once after TARGETS is set) ----\n",
    "expanded_targets = print_full_targets(TARGETS, save_csv=True, out_dir=OUT_DIR)\n",
    "\n",
    "# ========= main loop (remove dup) =========\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    for T in TARGETS:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "        # Expand SNOMED outcome to include descendants (aliases); else None for non-SNOMED\n",
    "        aliases_codes, name_map = snomed_aliases_for_outcome(T)\n",
    "\n",
    "        # 1) k→T rows: allow concept_code == any alias (if SNOMED), else exact match\n",
    "        if aliases_codes:\n",
    "            k_to_T = _fetch_k_to_T_in(conn, sorted(aliases_codes))\n",
    "        else:\n",
    "            k_to_T = _fetch_k_to_T(conn, T)\n",
    "\n",
    "        # --- DEBUG: alias breakdown BEFORE canonicalization ---\n",
    "        if aliases_codes and not k_to_T.empty:\n",
    "            present_mask = k_to_T[\"concept_code\"].isin(aliases_codes)\n",
    "            k_to_T_alias = k_to_T.loc[present_mask].copy()\n",
    "\n",
    "            alias_break = (\n",
    "                k_to_T_alias\n",
    "                .groupby(\"concept_code\", as_index=False)\n",
    "                .agg(\n",
    "                    rows=(\"concept_code\", \"size\"),\n",
    "                    n_code_target=(\"n_code_target\", \"sum\"),\n",
    "                    n_code_no_target=(\"n_code_no_target\", \"sum\"),\n",
    "                    n_target=(\"n_target\", \"sum\"),\n",
    "                    n_no_target=(\"n_no_target\", \"sum\"),\n",
    "                )\n",
    "                .sort_values(\"rows\", ascending=False)\n",
    "            )\n",
    "\n",
    "            present_aliases = set(alias_break[\"concept_code\"])\n",
    "            print(f\"[ALIASES] total_possible={len(aliases_codes)}  present_in_db={len(present_aliases)}\")\n",
    "            print(f\"[ALIASES] sample_present: {', '.join(list(present_aliases)[:10])}\"\n",
    "                f\"{' ...' if len(present_aliases) > 10 else ''}\")\n",
    "\n",
    "            alias_csv = os.path.join(OUT_DIR, f\"alias_breakdown_{T}.csv\")\n",
    "            alias_break.to_csv(alias_csv, index=False)\n",
    "            print(f\"[SAVED] Alias breakdown → {alias_csv}\")\n",
    "\n",
    "        # --- Canonicalize concept_code to the root outcome T (name_map replacement) ---\n",
    "        # This collapses all aliases back to T so MAGI has a single outcome node.\n",
    "        if not k_to_T.empty and name_map:\n",
    "            k_to_T[\"concept_code\"] = k_to_T[\"concept_code\"].replace(name_map)\n",
    "\n",
    "        # --- NEW: drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(k_to_T)\n",
    "        k_to_T = k_to_T.drop_duplicates()\n",
    "        after = len(k_to_T)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] k→T exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        if k_to_T.empty:\n",
    "            print(f\"[WARN] No k→T rows for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2) derive totals/effects on the k→T list\n",
    "        k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "        # 3) select predictors: top-K by total_effect (one row per k)\n",
    "        sel_rows = _select_top_by_te_unique_k(k_to_T, top_k=TOP_K)\n",
    "        if sel_rows.empty:\n",
    "            print(f\"[WARN] No predictors selected for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        selected_k = set(sel_rows[\"target_concept_code\"].astype(str))\n",
    "        print(f\"[SELECT] unique k available={k_to_T['target_concept_code'].nunique():,}  \"\n",
    "              f\"selected={len(selected_k):,}\")\n",
    "\n",
    "        # 4) build subgraph among {T} ∪ selected_k\n",
    "        events_set = selected_k | {T}\n",
    "        df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "\n",
    "        # Allow concept_code to be either events_set OR (if SNOMED) the aliases\n",
    "        allowed_right = events_set if not aliases_codes else (events_set | aliases_codes)\n",
    "        df_trim = df_trim[\n",
    "            df_trim[\"target_concept_code\"].isin(events_set) &\n",
    "            df_trim[\"concept_code\"].isin(allowed_right)\n",
    "        ].copy()\n",
    "\n",
    "        # Canonicalize alias outcomes → T inside the subgraph before analysis\n",
    "        # --- AUDIT: preserve original alias in subgraph CSV ---\n",
    "        if not df_trim.empty and name_map:\n",
    "            df_trim[\"concept_code_original\"] = df_trim[\"concept_code\"]\n",
    "            df_trim[\"concept_code\"] = df_trim[\"concept_code\"].replace(name_map)\n",
    "\n",
    "        # --- NEW: drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(df_trim)\n",
    "        df_trim = df_trim.drop_duplicates()\n",
    "        after = len(df_trim)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        # Recompute derived cols (incl. total_effect) on the subgraph\n",
    "        df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "        # Reporting (count k→T using aliases if applicable)\n",
    "        k_to_T_count = (df_trim[\"concept_code\"] == T).sum()\n",
    "        T_to_j_count = (df_trim[\"target_concept_code\"] == T).sum()\n",
    "        print(f\"[TRIM] rows={len(df_trim):,}  events={len(events_set)}  \"\n",
    "              f\"k→T rows={k_to_T_count}  T→j rows={T_to_j_count}\")\n",
    "\n",
    "        # 5) save subgraph (for audit)\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "        # 6) run MAGI — pass name_map so analysis sees only the canonical outcome T\n",
    "        try:\n",
    "            res = analyze_causal_sequence_py(df_trim, events=None, name_map=name_map or None, force_outcome=T)\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] MAGI failed:\", e)\n",
    "            continue\n",
    "\n",
    "        # 7) save NON-ZERO coefficients only\n",
    "        outcome_used = res.get(\"order_used\", [T])[-1]\n",
    "        coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "        # robust: accept 'coef' or 'beta' column name\n",
    "        coef_col = \"coef\" if \"coef\" in coef_df.columns else (\"beta\" if \"beta\" in coef_df.columns else None)\n",
    "        if coef_col is None:\n",
    "            raise KeyError(\"Coefficient column not found in coef_df (expected 'coef' or 'beta').\")\n",
    "\n",
    "        # filter to non-zero (tolerance to avoid tiny numerical noise)\n",
    "        eps = 1e-12\n",
    "        mask_nz = coef_df[coef_col].astype(float).abs() > eps\n",
    "        coef_nz = coef_df.loc[mask_nz].copy()\n",
    "\n",
    "        coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}_nonzero.csv\")\n",
    "        coef_nz.to_csv(coef_csv, index=False)\n",
    "\n",
    "        print(f\"[SAVED] Coefficients (non-zero only) → {coef_csv}  \"\n",
    "              f\"| kept={len(coef_nz):,} of {len(coef_df):,}  \"\n",
    "              f\"| antecedents={len(res.get('order_used', [])) - 1}  \"\n",
    "              f\"| used_total_effect={res.get('used_total_effect', True)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
