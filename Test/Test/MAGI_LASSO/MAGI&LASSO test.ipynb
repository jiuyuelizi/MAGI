{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4464f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import math\n",
    "from typing import Dict, List, Union, Any, Optional, Set, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, re\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e579f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIG =====\n",
    "MAGI_DB_PATH = os.getenv(\"MAGI_DB_PATH\", \"/projects/klybarge/pcori_ad/magi/magi_db/magi.db\")  # read-only; override via env\n",
    "OUT_DIR = \"./MAGI_LASSO\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "EDGE_TABLE = \"magi_counts_top500\"   \n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "\n",
    "TOP_K = 1900  # 500 for others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37322635",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = [\n",
    "    \"aa_meas_amitriptyline_rem\",\n",
    "    \"aa_meas_fluoxetine_rem\", \n",
    "    \"aa_meas_citalopram_rem\",\n",
    "    \"aa_meas_venlafaxine_rem\",\n",
    "    \"aa_meas_mirtazapine_rem\",\n",
    "    \"aa_meas_sertraline_rem\",\n",
    "    \"aa_meas_bupropion_rem\",\n",
    "    \"aa_meas_trazodone_rem\",\n",
    "    \"aa_meas_duloxetine_rem\",\n",
    "    \"aa_meas_escitalopram_rem\",\n",
    "    \"aa_meas_paroxetine_rem\",\n",
    "    \"aa_meas_nortriptyline_rem\",\n",
    "    \"aa_meas_other_rem\",\n",
    "    \"aa_meas_doxepin_rem\",\n",
    "    \"aa_meas_desvenlafaxine_rem\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fadfd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TARGETS = [\n",
    "    \"aa_meas_citalopram_rem\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3516458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "[RUN] Target = aa_meas_citalopram_rem\n",
      "[SELECT] total unique k=4,148  risk=3,615  prot=66  selected(total)=1016  with: risk=950, prot=66\n",
      "[SAVED] Factors → ./MAGI_LASSO/risk_prot_aa_meas_citalopram_rem.csv\n",
      "[SELECT] unique k available=4,148  selected=1,016\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\n      SELECT m.*,\n             tcn.concept_code AS target_concept_code,   -- LEFT\n             ccn.concept_code AS concept_code           -- RIGHT\n      FROM magi_counts_top500 m\n      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n      WHERE tcn.concept_code IN (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n    ': too many SQL variables",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/pcoridev/lib64/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: too many SQL variables",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bfb256b95e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# 4) build subgraph among {T} ∪ selected_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mevents_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_k\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mdf_trim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fetch_subgraph_by_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         df_trim = df_trim[\n\u001b[1;32m    464\u001b[0m             \u001b[0mdf_trim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_concept_code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bfb256b95e73>\u001b[0m in \u001b[0;36m_fetch_subgraph_by_targets\u001b[0;34m(conn, events_list)\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0mWHERE\u001b[0m \u001b[0mtcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcept_code\u001b[0m \u001b[0mIN\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mph\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \"\"\"\n\u001b[0;32m--> 419\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;31m# ========= main loop =========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pcoridev/lib64/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     )\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pcoridev/lib64/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pcoridev/lib64/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution failed on sql '{args[0]}': {exc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '\n      SELECT m.*,\n             tcn.concept_code AS target_concept_code,   -- LEFT\n             ccn.concept_code AS concept_code           -- RIGHT\n      FROM magi_counts_top500 m\n      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n      WHERE tcn.concept_code IN (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n    ': too many SQL variables"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Update 10 15 25\n",
    "def analyze_causal_sequence_py(\n",
    "    df_in: Union[str, pd.DataFrame],\n",
    "    *,\n",
    "    name_map: Dict[str, str] = None,     # raw_code -> friendly label (applied to BOTH columns)\n",
    "    events: List[str] = None,            # event names to KEEP (AFTER recoding). If None: auto-detect\n",
    "    force_outcome: str = None,           # if provided and present, force this to be the FINAL node (Y)\n",
    "    lambda_min_count: int = 15           # L-threshold for λ: if n_code < L ⇒ λ_{k,j}=0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    MAGI (Python): Reference routine with explicit comments.\n",
    "\n",
    "    ─────────────────────────────────────────────────────────────────────────────\n",
    "    COLUMN CONVENTION PER ROW:\n",
    "      Left  column: target_concept_code  (call this X for this row)\n",
    "      Right column: concept_code         (call this k for this row)\n",
    "\n",
    "      n_code_target        ≡  k ∧ X\n",
    "      n_code_no_target     ≡  k ∧ ¬X\n",
    "      n_target_no_code     ≡  ¬k ∧ X\n",
    "      n_no_target          ≡  total(¬X)\n",
    "      n_target             ≡  total(X)\n",
    "      n_code               ≡  total(k)\n",
    "      n_code_before_target ≡  count(k before X)\n",
    "      n_target_before_code ≡  count(X before k)\n",
    "\n",
    "    ORIENTATION (locked to your spec):\n",
    "      • Total effect T_{kY}:   read row (target = Y, code = k).\n",
    "      • Lambda     λ_{k,j}:    read row (target = j, code = k), and compute\n",
    "                               λ_{k,j} = n_code_target / n_code  with L-threshold on n_code.\n",
    "\n",
    "    TEMPORAL SCORE for each node Zi:\n",
    "      Score(Z_i) = Σ_{j≠i} [ C(Z_i≺Z_j) - C(Z_j≺Z_i) + C(Z_i∧¬Z_j) - C(Z_j∧¬Z_i) ]\n",
    "      Read from row (target=Z_j, code=Z_i):\n",
    "        - n_code_before_target      → C(Z_i≺Z_j)\n",
    "        - n_target_before_code      → C(Z_j≺Z_i)\n",
    "        - n_code_no_target          → C(Z_i∧¬Z_j)\n",
    "        - n_target_no_code          → C(Z_j∧¬Z_i)\n",
    "\n",
    "    T_{kY} from row (Y, k):\n",
    "      a = n_code_target        (k ∧ Y)\n",
    "      b = n_code_no_target     (k ∧ ¬Y)\n",
    "      c = n_target_no_code     (¬k ∧ Y)\n",
    "      d = n_no_target - b      (¬k ∧ ¬Y)   ← computed on the fly (no extra column needed)\n",
    "      With sample-size–anchored odds:\n",
    "         odds_k1 = a/b with guards; odds_k0 = c/d with guards; T = odds_k1 / odds_k0\n",
    "\n",
    "    DIRECT EFFECTS via backward recursion:\n",
    "      D_{k,Y} = ( T_{k,Y} - Σ_j λ_{k,j} D_{j,Y} ) / ( 1 - Σ_j λ_{k,j} ),\n",
    "      where j are downstream nodes between k and Y in the temporal order.\n",
    "\n",
    "    LOGISTIC LINK:\n",
    "      logit P(Y=1 | Z) = β0 + Σ_k β_k Z_k  with β_k = log D_{k,Y};\n",
    "      invalid/nonpositive D map to β_k=0.\n",
    "\n",
    "    RETURNS a dict with:\n",
    "      sorted_scores, temporal_order, order_used,\n",
    "      T_val, D_val, lambda_l, coef_df, beta_0, beta, logit_predictors, predict_proba\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 0) Ingest & validate ───────────────────────────────────────────────────\n",
    "    df = pd.read_csv(df_in) if isinstance(df_in, str) else df_in.copy()\n",
    "\n",
    "    need_cols = [\n",
    "        \"target_concept_code\", \"concept_code\",\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\",\n",
    "        \"n_target_no_code\",\n",
    "        \"n_code\",                                 # for λ denominator\n",
    "        \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    # Optional recoding to friendly labels (applied to BOTH endpoints)\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "        df[\"concept_code\"]        = df[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    # Limit to selected events (union if auto)\n",
    "    if events is None:\n",
    "        ev_t = df[\"target_concept_code\"].astype(str).unique().tolist()\n",
    "        ev_c = df[\"concept_code\"].astype(str).unique().tolist()\n",
    "        events = sorted(set(ev_t) | set(ev_c))\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events.\")\n",
    "\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "\n",
    "    # Coerce numerics; NA→0 to allow safe sums/max\n",
    "    num_cols = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target\", \"n_no_target\", \"n_target_no_code\",\n",
    "        \"n_code\", \"n_code_before_target\", \"n_target_before_code\",\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # ── 1) Temporal score (read rows: target=Z_j, code=Z_i) ────────────────────\n",
    "    scores: Dict[str, float] = {}\n",
    "    for zi in events:\n",
    "        s = 0.0\n",
    "        for zj in (x for x in events if x != zi):\n",
    "            # Row oriented as (Zj, Zi)\n",
    "            pair = df[(df[\"target_concept_code\"] == zj) & (df[\"concept_code\"] == zi)]\n",
    "            if pair.empty:\n",
    "                continue\n",
    "            c_i_before_j = float(pair[\"n_code_before_target\"].sum(skipna=True))   # Zi before Zj\n",
    "            c_j_before_i = float(pair[\"n_target_before_code\"].sum(skipna=True))   # Zj before Zi\n",
    "            c_i_and_not_j = float(pair[\"n_code_no_target\"].sum(skipna=True))      # Zi ∧ ¬Zj\n",
    "            c_j_and_not_i = float(pair[\"n_target_no_code\"].sum(skipna=True))      # Zj ∧ ¬Zi\n",
    "            s += (c_i_before_j - c_j_before_i + c_i_and_not_j - c_j_and_not_i)\n",
    "        scores[zi] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Choose outcome Y: either forced or top-scoring node\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "    else:\n",
    "        outcome = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome] + [outcome]\n",
    "\n",
    "    events_order = temporal_order              # earliest … → Y\n",
    "    antecedents = events_order[:-1]            # everything before Y\n",
    "\n",
    "    # ── 2) T and λ (row orientations locked) ───────────────────────────────────\n",
    "    T_val = pd.Series(0.0, index=antecedents, dtype=float)\n",
    "    D_val = pd.Series(np.nan, index=antecedents, dtype=float)\n",
    "    lambda_l: Dict[str, pd.Series] = {}\n",
    "\n",
    "    for k in antecedents:\n",
    "        # ---- T_{kY} from the single row (target=Y, code=k) ----\n",
    "        row_Yk = df[(df[\"target_concept_code\"] == outcome) & (df[\"concept_code\"] == k)]\n",
    "\n",
    "        # 2×2 cells (a,b,c,d) at (Y,k):\n",
    "        a = float(row_Yk[\"n_code_target\"].sum(skipna=True))        # k ∧ Y\n",
    "        b = float(row_Yk[\"n_code_no_target\"].sum(skipna=True))     # k ∧ ¬Y\n",
    "        c = float(row_Yk[\"n_target_no_code\"].sum(skipna=True))     # ¬k ∧ Y\n",
    "        # d is not in data; compute from the same row: d = total(¬Y) − (k ∧ ¬Y)\n",
    "        n_noY = float(row_Yk[\"n_no_target\"].max(skipna=True)) if not row_Yk.empty else 0.0\n",
    "        d = max(n_noY - b, 0.0)                                    # ¬k ∧ ¬Y\n",
    "\n",
    "        # Stratum sizes\n",
    "        N1 = a + b                   # k = 1\n",
    "        N0 = c + d                   # k = 0\n",
    "\n",
    "        # Sample-size–anchored odds for k=1 and k=0\n",
    "        if N1 == 0:\n",
    "            odds_k1 = 1.0\n",
    "        else:\n",
    "            if a == 0:\n",
    "                odds_k1 = 1.0 / (N1 + 1.0)\n",
    "            elif b == 0:\n",
    "                odds_k1 = (N1 + 1.0)\n",
    "            else:\n",
    "                odds_k1 = a / b\n",
    "\n",
    "        if N0 == 0:\n",
    "            odds_k0 = 1.0\n",
    "        else:\n",
    "            if c == 0:\n",
    "                odds_k0 = 1.0 / (N0 + 1.0)\n",
    "            elif d == 0:\n",
    "                odds_k0 = (N0 + 1.0)\n",
    "            else:\n",
    "                odds_k0 = c / d\n",
    "\n",
    "        T_val.loc[k] = float(odds_k1 / odds_k0) if odds_k0 > 0 else (N1 + 1.0)\n",
    "\n",
    "        # ---- λ_{k,j} from rows (target=j, code=k): λ = n_code_target / n_code ----\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = []\n",
    "        for j in js:\n",
    "            row_jk = df[(df[\"target_concept_code\"] == j) & (df[\"concept_code\"] == k)]\n",
    "            if row_jk.empty:\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            num = float(pd.to_numeric(row_jk[\"n_code_target\"], errors=\"coerce\").fillna(0.0).sum())\n",
    "            den = float(pd.to_numeric(row_jk[\"n_code\"],        errors=\"coerce\").fillna(0.0).sum())\n",
    "\n",
    "            # L-threshold on the conditioning size n_code (count of k)\n",
    "            if (den <= 0) or (den < lambda_min_count):\n",
    "                lam_pairs.append((j, 0.0))\n",
    "                continue\n",
    "\n",
    "            lam = num / den\n",
    "            lam = 0.0 if not np.isfinite(lam) else float(min(max(lam, 0.0), 1.0))\n",
    "            lam_pairs.append((j, lam))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_pairs}, dtype=float)\n",
    "\n",
    "    # ── 3) Backward recursion for direct effects D ─────────────────────────────\n",
    "    # Last antecedent (just before Y): no downstream → D = T\n",
    "    if len(antecedents) >= 1:\n",
    "        last_anc = antecedents[-1]\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "\n",
    "    # Walk backward for the rest\n",
    "    if len(antecedents) > 1:\n",
    "        for k in list(reversed(antecedents[:-1])):\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            downstream = list(lam_vec.index)  # j nodes after k (already resolved)\n",
    "            lam_vals = lam_vec.reindex(downstream).fillna(0.0).to_numpy()\n",
    "            D_down  = pd.to_numeric(D_val.reindex(downstream), errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vals * D_down))\n",
    "            den = 1.0 - float(np.nansum(lam_vals))\n",
    "\n",
    "            if (not np.isfinite(den)) or den == 0.0:\n",
    "                D_val.loc[k] = T_val.loc[k]            # neutralize if pathological\n",
    "            else:\n",
    "                tmp = num / den\n",
    "                D_val.loc[k] = tmp if np.isfinite(tmp) else T_val.loc[k]\n",
    "\n",
    "    # ── 4) Logistic link (β) and predict_proba ─────────────────────────────────\n",
    "    # Intercept β0 from marginal prevalence of Y (rows with target == Y)\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    n_t = float(resp_rows[\"n_target\"].max(skipna=True)) if not resp_rows.empty else np.nan\n",
    "    n_n = float(resp_rows[\"n_no_target\"].max(skipna=True)) if not resp_rows.empty else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))\n",
    "\n",
    "    # β_k = log D_{k,Y}; invalid/nonpositive → 0\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    beta_vals = np.log(D_clean.where(D_clean > 0.0)).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_vals.index) + [\"(intercept)\"],\n",
    "        \"beta\":      list(beta_vals.values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    # Vectorized predict_proba\n",
    "    predictors = list(beta_vals.index)\n",
    "    beta_vec = beta_vals.values\n",
    "\n",
    "    def predict_proba(Z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]):\n",
    "        \"\"\"\n",
    "        Compute P(Y=1|Z) using: logit P = β0 + Σ_k β_k Z_k.\n",
    "        Z can be:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame containing any/all of `predictors` (others ignored)\n",
    "        \"\"\"\n",
    "        def sigmoid(x):\n",
    "            x = np.clip(x, -700, 700)  # numerical stability for large |η|\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "        if isinstance(Z, pd.DataFrame):\n",
    "            M = Z.reindex(columns=predictors, fill_value=0.0).astype(float).to_numpy()\n",
    "            return sigmoid(beta_0 + M @ beta_vec)\n",
    "\n",
    "        if isinstance(Z, (dict, pd.Series)):\n",
    "            v = np.array([float(Z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            return float(sigmoid(beta_0 + float(v @ beta_vec)))\n",
    "\n",
    "        arr = np.asarray(Z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            return float(sigmoid(beta_0 + float(arr @ beta_vec)))\n",
    "        if arr.ndim == 2:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*,{len(predictors)}), got {arr.shape}\")\n",
    "            return sigmoid(beta_0 + arr @ beta_vec)\n",
    "\n",
    "        raise ValueError(\"Unsupported input for predict_proba\")\n",
    "\n",
    "    # ── 5) Package results ─────────────────────────────────────────────────────\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n",
    "\n",
    "def _ensure_derived_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Only coerce numerics and compute total_effect if absent, using provided cells and\n",
    "    computing d = n_no_target - n_code_no_target on the fly (since d isn't in data).\n",
    "    \"\"\"\n",
    "    required = [\n",
    "        \"n_code_target\", \"n_code_no_target\",\n",
    "        \"n_target_no_code\", \"n_target\", \"n_no_target\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns for TE: {', '.join(missing)}\")\n",
    "\n",
    "    out = df.copy()\n",
    "    for c in required:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    if \"total_effect\" not in out.columns:\n",
    "        a = out[\"n_code_target\"].astype(float)\n",
    "        b = out[\"n_code_no_target\"].astype(float)\n",
    "        c = out[\"n_target_no_code\"].astype(float)\n",
    "        d = (out[\"n_no_target\"] - out[\"n_code_no_target\"]).astype(float)  # compute d locally\n",
    "\n",
    "        N1 = a + b\n",
    "        N0 = c + d\n",
    "\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            odds_k1 = np.where(\n",
    "                N1 == 0, 1.0,\n",
    "                np.where((a > 0) & (b > 0), a / b,\n",
    "                         np.where(b == 0, N1 + 1.0, 1.0 / (N1 + 1.0)))\n",
    "            )\n",
    "            odds_k0 = np.where(\n",
    "                N0 == 0, 1.0,\n",
    "                np.where((c > 0) & (d > 0), c / d,\n",
    "                         np.where(d == 0, N0 + 1.0, 1.0 / (N0 + 1.0)))\n",
    "            )\n",
    "            te = odds_k1 / odds_k0\n",
    "\n",
    "        out[\"total_effect\"] = np.where(np.isfinite(te), te, 1.0).astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# TOPK_HI/LO\n",
    "def _select_top_by_te_unique_k(k_to_T: pd.DataFrame, top_k: int = TOP_K,\n",
    "                               hi: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balanced selection without fallback:\n",
    "      - Take up to top_k//2 strongest risk (TE >= hi)\n",
    "      - Take up to top_k - top_k//2 strongest protective (TE <= 1/hi)\n",
    "      - If either side is short, do NOT fill from elsewhere; total may be < top_k\n",
    "      - Return one row per k (**concept_code**), ranked by extremeness\n",
    "    \"\"\"\n",
    "    df = k_to_T.copy()\n",
    "\n",
    "    # Ensure total_effect numeric\n",
    "    df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effect\"]).copy()\n",
    "    if df.empty:\n",
    "        print(\"[SELECT] no rows with total_effect; returning empty selection.\")\n",
    "        return df\n",
    "\n",
    "    # Extremeness symmetrical around 1\n",
    "    df[\"effect_strength\"] = np.where(df[\"total_effect\"] >= 1.0,\n",
    "                                     df[\"total_effect\"],\n",
    "                                     1.0 / df[\"total_effect\"])\n",
    "\n",
    "    # One row per k: **k is concept_code in (target=Y, code=k)**\n",
    "    best_per_k = (df.sort_values(\"effect_strength\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"concept_code\"], keep=\"first\"))\n",
    "\n",
    "    HI = hi\n",
    "    LO = 1.0 / hi\n",
    "\n",
    "    risk_pool = best_per_k[best_per_k[\"total_effect\"] >= HI].copy()\n",
    "    prot_pool = best_per_k[best_per_k[\"total_effect\"] <= LO].copy()\n",
    "\n",
    "    want_risk = top_k // 2\n",
    "    want_prot = top_k - want_risk\n",
    "\n",
    "    sel_risk = risk_pool.nlargest(min(len(risk_pool), want_risk), \"effect_strength\")\n",
    "    sel_prot = prot_pool.nlargest(min(len(prot_pool), want_prot), \"effect_strength\")\n",
    "\n",
    "    selected = pd.concat([sel_risk, sel_prot], ignore_index=True)\n",
    "\n",
    "    print(f\"[SELECT] total unique k={best_per_k['concept_code'].nunique():,}  \"\n",
    "          f\"risk={len(risk_pool):,}  prot={len(prot_pool):,}  \"\n",
    "          f\"selected(total)={selected['concept_code'].nunique()}  \"\n",
    "          f\"with: risk={len(sel_risk)}, prot={len(sel_prot)}\")\n",
    "\n",
    "    return selected.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _fetch_k_to_T(conn, outcome_code: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch all rows whose LEFT (target) == outcome Y.\n",
    "    These rows provide the single (Y, k) lines needed to compute T_{kY}.\n",
    "    \"\"\"\n",
    "    q = \"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- LEFT  (Y)\n",
    "             ccn.concept_code AS concept_code           -- RIGHT (k)\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code = ?                         -- <<< filter on LEFT == Y\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[outcome_code])\n",
    "\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list):\n",
    "    \"\"\"\n",
    "    Fetch the induced subgraph for the event set on the LEFT side.\n",
    "    This captures all rows (X, k) where X ∈ events_set and k ∈ events_set,\n",
    "    which includes:\n",
    "      • (Y, k) rows (needed for T_{kY})\n",
    "      • (j, k) rows (needed for λ_{k,j})\n",
    "    \"\"\"\n",
    "    ph = \",\".join([\"?\"] * len(events_list))\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- LEFT\n",
    "             ccn.concept_code AS concept_code           -- RIGHT\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=list(events_list))\n",
    "\n",
    "# ========= main loop =========\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    for T in TARGETS:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "        # 1) Y→k rows (target == T) – used for T_{kY}\n",
    "        k_to_T = _fetch_k_to_T(conn, T)\n",
    "\n",
    "        # --- NEW: drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(k_to_T)\n",
    "        k_to_T = k_to_T.drop_duplicates()\n",
    "        after = len(k_to_T)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] k_to_T exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        if k_to_T.empty:\n",
    "            print(f\"[WARN] No predictor→target rows for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2) derive totals/effects on the k→T list\n",
    "        k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "        # 3) select predictors: top-K by total_effect (one row per k = concept_code)\n",
    "        sel_rows = _select_top_by_te_unique_k(k_to_T, top_k=TOP_K)\n",
    "        if sel_rows.empty:\n",
    "            print(f\"[WARN] No predictors selected for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"risk_prot_{T}.csv\")\n",
    "        sel_rows.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Factors → {sub_csv}\")\n",
    "\n",
    "        # IMPORTANT: predictors are the **RIGHT** side (concept_code) in (target=Y, code=k)\n",
    "        selected_k = set(sel_rows[\"concept_code\"].astype(str))\n",
    "        print(f\"[SELECT] unique k available={k_to_T['concept_code'].nunique():,}  \"\n",
    "            f\"selected={len(selected_k):,}\")\n",
    "\n",
    "        # 4) build subgraph among {T} ∪ selected_k\n",
    "        events_set = selected_k | {T}\n",
    "        df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "        df_trim = df_trim[\n",
    "            df_trim[\"target_concept_code\"].isin(events_set) &\n",
    "            df_trim[\"concept_code\"].isin(events_set)\n",
    "        ].copy()\n",
    "\n",
    "        # --- NEW: drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(df_trim)\n",
    "        df_trim = df_trim.drop_duplicates()\n",
    "        after = len(df_trim)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        # Recompute derived cols (incl. total_effect) on the subgraph\n",
    "        df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "        # Correct orientation in the prints:\n",
    "        #   k→T rows: concept_code == T\n",
    "        #   T→j rows: target_concept_code == T\n",
    "        print(\n",
    "            f\"[TRIM] rows={len(df_trim):,}  events={len(events_set)}  \"\n",
    "            f\"Y→k rows (target=={T}) = {(df_trim['target_concept_code'] == T).sum()}  \"  # was concept_code==T\n",
    "            f\"j→k rows (target in events_set, ≠{T}) = \"\n",
    "            f\"{(df_trim['target_concept_code'].isin(events_set) & (df_trim['target_concept_code'] != T)).sum()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # 5) save subgraph (for audit)\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "        # 6) run MAGI\n",
    "        try:\n",
    "            res = analyze_causal_sequence_py(df_trim, events=None, name_map=None, force_outcome=T)\n",
    "        except TypeError:\n",
    "            res = analyze_causal_sequence_py(df_trim)\n",
    "            outcome_used = res.get(\"order_used\", [None])[-1]\n",
    "            if outcome_used != T:\n",
    "                print(f\"[NOTE] outcome auto-inferred as {outcome_used}, not {T}\")\n",
    "\n",
    "        # 7) save NON-ZERO coefficients only\n",
    "        outcome_used = res.get(\"order_used\", [T])[-1]\n",
    "        coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "        # robust: accept 'coef' or 'beta' column name\n",
    "        coef_col = \"coef\" if \"coef\" in coef_df.columns else (\"beta\" if \"beta\" in coef_df.columns else None)\n",
    "        if coef_col is None:\n",
    "            raise KeyError(\"Coefficient column not found in coef_df (expected 'coef' or 'beta').\")\n",
    "\n",
    "        # filter to non-zero (tolerance to avoid tiny numerical noise)\n",
    "        eps = 1e-12\n",
    "        mask_nz = coef_df[coef_col].astype(float).abs() > eps\n",
    "        coef_nz = coef_df.loc[mask_nz].copy()\n",
    "\n",
    "        coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}_nonzero.csv\")\n",
    "        coef_nz.to_csv(coef_csv, index=False)\n",
    "\n",
    "        print(f\"[SAVED] Coefficients (non-zero only) → {coef_csv}  \"\n",
    "              f\"| kept={len(coef_nz):,} of {len(coef_df):,}  \"\n",
    "              f\"| antecedents={len(res.get('order_used', [])) - 1}  \"\n",
    "              f\"| used_total_effect={res.get('used_total_effect', True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Read the counts table\n",
    "    # Use a raw string for backslashes on Windows\n",
    "    df = pd.read_csv(os.path.join(f\"output.csv\"))\n",
    "\n",
    "    # 2) Map raw codes -> friendly labels\n",
    "    name_map = {\n",
    "        \"px_CPT4_0025T\":      \"Cornea\",\n",
    "        \"dx_SNOMED_47639008\": \"Cyst\",\n",
    "        \"px_CPT4_00840\":      \"Anesthesia\",\n",
    "        \"aa_meas_citalopram\": \"Response\",\n",
    "    }\n",
    "\n",
    "    # 3) Events to keep (AFTER recoding)\n",
    "    events = [\"Cornea\", \"Cyst\", \"Anesthesia\", \"Response\"]\n",
    "\n",
    "    # 4) Run the analysis with different L thresholds\n",
    "    for L in (30, 15, 10):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"[RUN] lambda_min_count = {L}\")\n",
    "        res = analyze_causal_sequence_py(\n",
    "            df, name_map=name_map, events=events, force_outcome=None, lambda_min_count=L\n",
    "        )\n",
    "        print(\"[ORDER]\", \" -> \".join(res[\"temporal_order\"]))\n",
    "        print(\"[T]\");   print(res[\"T_val\"].round(6))\n",
    "        print(\"[D]\");   print(res[\"D_val\"].round(6))\n",
    "        print(\"[β]\");   print(res[\"coef_df\"])\n",
    "\n",
    "        # Example: inspect λ for one antecedent if present\n",
    "        if len(res[\"order_used\"]) >= 2:\n",
    "            k0 = res[\"order_used\"][0]\n",
    "            lam0 = res[\"lambda_l\"].get(k0)\n",
    "            if lam0 is not None and len(lam0) > 0:\n",
    "                print(f\"[λ for {k0}]\")\n",
    "                print(lam0.round(6))\n",
    "\n",
    "    # 5) Example: compute probabilities for a simple feature vector\n",
    "    #    (set Cornea=1, Cyst=0, Anesthesia=1 for demonstration)\n",
    "    demo = {\"Cornea\": 1, \"Cyst\": 1, \"Anesthesia\": 1}\n",
    "    res_demo = analyze_causal_sequence_py(\n",
    "        df, name_map=name_map, events=events, lambda_min_count=30\n",
    "    )\n",
    "    p = res_demo[\"predict_proba\"](demo)\n",
    "    print(\"\\n[DEMO predict_proba] with\", demo, \"→ P(Response=1) =\", round(float(p), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f36bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "############### MAGI FUNCTION ###############\n",
    "\"\"\"\n",
    "MAGI: Dependent Bayes with Temporal Ordering — Reference Implementation\n",
    "-------------------------------------------------------------------------------\n",
    "This file adds **step-by-step comments** to the function `analyze_causal_sequence_py`. The comments mirror the proposed\n",
    "algorithm sections:\n",
    "\n",
    "1) Determination of Temporal Order\n",
    "2) Estimation of Dependent Bayes (T-values, λ-links, and D-values)\n",
    "3) Logistic link to produce P(Y=1 | Z)\n",
    "\n",
    "INPUT TABLE EXPECTATIONS (long format, one row per directed pair):\n",
    "- target_concept_code : str (# left node in the edge; in many places this\n",
    "denotes the earlier event)\n",
    "- concept_code : str (# right node in the edge; the later event)\n",
    "- n_code_target : float/int (# co-occurrence count for concept & target\n",
    "in the *target=Y* stratum; used as 'a')\n",
    "- n_code_no_target : float/int (# co-occurrence count for concept & target\n",
    "in the *target=¬Y* stratum; used as 'b')\n",
    "- n_target : float/int (# total Y count for this target concept, per edge row; we take max within a block)\n",
    "- n_no_target : float/int (# total ¬Y count; max within a block)\n",
    "- no_code_no_target : float/int (# optional, computed if missing as n_no_target - n_code_no_target; clipped ≥0)\n",
    "- n_target_before_code: float/int (# count where target occurred before code)\n",
    "- n_code_before_target: float/int (# count where code occurred before target)\n",
    "\n",
    "NOTES ON TEMPORAL COUNTS:\n",
    "For a row with target_concept_code = A and concept_code = B, the columns\n",
    "`n_code_before_target` and `n_target_before_code` are interpreted as:\n",
    "- n_code_before_target: # of persons where **B happened before A**\n",
    "- n_target_before_code: # of persons where **A happened before B**\n",
    "We aggregate these across j≠i to compute *temporal scores* for each event.\n",
    "\n",
    "PIECEWISE, SAMPLE-SIZE–ANCHORED ADJUSTMENTS:\n",
    "- For odds terms that would be 0 or ∞ due to zero cells, we replace the\n",
    "offending odds with 1/(N+1) or (N+1)/1, where N is the size of the\n",
    "appropriate stratum, so all ratios remain finite and interpretable.\n",
    "\n",
    "RETURN VALUE:\n",
    "A dict with temporal ordering, T-values, λ-vectors, D-values, coefficients\n",
    "for a logistic link, a `predict_proba` callable, and trace tables.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_causal_sequence_py(\n",
    "    data: Union[str, pd.DataFrame],\n",
    "    name_map: Dict[str, str],\n",
    "    events: List[str],\n",
    "    force_outcome=None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Compute temporal order, dependent-Bayes direct effects (D), and\n",
    "    a logistic-link probability for outcome Y from pairwise counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str | DataFrame\n",
    "        CSV path or in-memory DataFrame with the columns described above.\n",
    "    name_map : Dict[str, str]\n",
    "        Optional mapping raw code -> friendly label. If provided, both\n",
    "        `target_concept_code` and `concept_code` are replaced.\n",
    "    events : List[str]\n",
    "        List of event names/codes to restrict the analysis to. If `None`,\n",
    "        events are auto-detected from the table and intersected.\n",
    "    force_outcome : str | None\n",
    "        If provided and found among events, this event is forced to be the\n",
    "        **final** node (i.e., the outcome) in the temporal order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        - sorted_scores : pd.Series of temporal scores (desc)\n",
    "        - temporal_order: list of events (outcome at the end)\n",
    "        - order_used    : same as temporal_order\n",
    "        - T_val         : pd.Series of total effects T_{k,Y}\n",
    "        - D_val         : pd.Series of direct effects D_{k,Y}\n",
    "        - coef_df       : pd.DataFrame of coefficients (β_k and intercept)\n",
    "        - lambda_l      : dict[str -> pd.Series] of λ_{k,j} vectors\n",
    "        - trace_df      : pd.DataFrame detailing the backward recursion steps\n",
    "        - invalid_predictors: list of predictors whose log(D) was invalid\n",
    "        - beta_0, beta, logit_predictors, predict_proba: logistic elements\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 0) INGEST & BASIC VALIDATION\n",
    "    # ---------------------------------------------------------------------\n",
    "    if isinstance(data, str):\n",
    "        # Read from CSV path\n",
    "        df = pd.read_csv(data)\n",
    "    else:\n",
    "        # Work on a copy to avoid mutating caller's object\n",
    "        df = data.copy()\n",
    "\n",
    "    # Ensure required identifier columns are present\n",
    "    for col in [\"target_concept_code\", \"concept_code\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Optional recoding to human-friendly labels\n",
    "    if name_map:\n",
    "        df[\"target_concept_code\"] = df[\"target_concept_code\"].replace(name_map)\n",
    "        df[\"concept_code\"] = df[\"concept_code\"].replace(name_map)\n",
    "\n",
    "    # Numeric columns the algorithm expects\n",
    "    need = [\n",
    "        \"n_code_target\", \"n_code_no_target\", \"n_target\", \"n_no_target\",\n",
    "        \"n_target_before_code\", \"n_code_before_target\",\n",
    "    ]\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "\n",
    "    # If precomputed total_effect exists (for λ or sanity checks), coerce to numeric\n",
    "    has_total = \"total_effect\" in df.columns\n",
    "    if has_total:\n",
    "        df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) EVENT SET (optional auto-detect) & TYPE COERCION\n",
    "    # ---------------------------------------------------------------------\n",
    "    if events is None:\n",
    "        # Auto-detect: intersect events appearing on both sides of edges\n",
    "        ev_targets = df[\"target_concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        ev_code = df[\"concept_code\"].dropna().astype(str).unique().tolist()\n",
    "        events = sorted(set(ev_targets).intersection(ev_code))\n",
    "        if len(events) == 0:\n",
    "            # Fall back to union if intersection is empty\n",
    "            events = sorted(set(ev_targets) | set(ev_code))\n",
    "    if len(events) < 2:\n",
    "        raise ValueError(\"Need at least two events after auto-detection.\")\n",
    "\n",
    "    # Keep only rows whose endpoints are both in the chosen event set\n",
    "    df = df[df[\"target_concept_code\"].isin(events) & df[\"concept_code\"].isin(events)].copy()\n",
    "\n",
    "    # Coerce numeric columns robustly (invalid -> NaN); subsequent ops handle NaNs\n",
    "    for c in need:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # If `no_code_no_target` missing, derive as (n_no_target - n_code_no_target) ≥ 0\n",
    "    if \"no_code_no_target\" not in df.columns:\n",
    "        df[\"no_code_no_target\"] = (df[\"n_no_target\"] - df[\"n_code_no_target\"]).clip(lower=0)\n",
    "    else:\n",
    "        df[\"no_code_no_target\"] = pd.to_numeric(df[\"no_code_no_target\"], errors=\"coerce\").clip(lower=0)\n",
    "\n",
    "    # Helper: total count for an event (max n_target where that event is target)\n",
    "    # This mirrors your original choice; change to .sum() if warranted.\n",
    "    def C_of(ev: str) -> float:\n",
    "        sub = df[df[\"target_concept_code\"] == ev]\n",
    "        if sub.empty:\n",
    "            return float(\"nan\")\n",
    "        C = pd.to_numeric(sub[\"n_target\"], errors=\"coerce\").max()\n",
    "        return float(C) if pd.notna(C) and np.isfinite(C) else float(\"nan\")\n",
    "\n",
    "    # Helper: numerically safe log (log(1)=0 for invalid/≤0)\n",
    "    def safe_log(x: float) -> float:\n",
    "        try:\n",
    "            xv = float(x)\n",
    "        except (TypeError, ValueError):\n",
    "            return 0.0\n",
    "        if not np.isfinite(xv) or xv <= 0.0:\n",
    "            return 0.0\n",
    "        return math.log(xv)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) TEMPORAL ORDER — pairwise-before counts → per-node score\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Score(Z_i) = Σ_{j≠i} [ C(Z_i≪Z_j) - C(Z_j≪Z_i) + C(Z_i ∩ ¬Z_j) - C(Z_j ∩ ¬Z_i) ]\n",
    "    # Here we implement the *before/after* portion using the provided columns.\n",
    "    # If only presence/absence terms are available, you may approximate using\n",
    "    # the last two terms.\n",
    "    scores: Dict[str, float] = {}\n",
    "    for zk in events:\n",
    "        s = 0.0\n",
    "        for zj in [x for x in events if x != zk]:\n",
    "            # For the pair (zj, zk), we interpret:\n",
    "            #   n_code_before_target   — # where zk (code) before zj (target)\n",
    "            #   n_target_before_code   — # where zj (target) before zk (code)\n",
    "            rowr = df[(df[\"target_concept_code\"] == zj) & (df[\"concept_code\"] == zk)]\n",
    "            if not rowr.empty:\n",
    "                s += float(rowr[\"n_code_before_target\"].sum(skipna=True) -\n",
    "                           rowr[\"n_target_before_code\"].sum(skipna=True))\n",
    "        scores[zk] = s\n",
    "\n",
    "    sorted_scores = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # Outcome selection:\n",
    "    #  - If `force_outcome` is provided and present, put it at the end.\n",
    "    #  - Else, default to the top-scoring node as outcome.\n",
    "    if force_outcome and (force_outcome in sorted_scores.index):\n",
    "        outcome_event = force_outcome\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "    else:\n",
    "        outcome_event = sorted_scores.index[0]\n",
    "        temporal_order = [ev for ev in sorted_scores.index if ev != outcome_event] + [outcome_event]\n",
    "\n",
    "    # Propagation order is the temporal order; last is outcome Y\n",
    "    events_order = temporal_order\n",
    "    outcome = events_order[-1]\n",
    "    antecedents = events_order[:-1]\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3) T-VALUES (TOTAL EFFECTS) & λ-LINKS BETWEEN ANTECEDENTS\n",
    "    # ---------------------------------------------------------------------\n",
    "    # For each antecedent k, we compute T_{k,Y} as an odds ratio between\n",
    "    # strata Z_k=1 and Z_k=0 with sample-size–anchored fixes for zero cells.\n",
    "    # For λ_{k,j} (dependence of j given k), we first use precomputed\n",
    "    # `total_effect` if present.\n",
    "    # If λ is missing (no recorded dependence), assume independence\n",
    "    # and treat the conditional contribution as 0 in the adjustment sum.\n",
    "\n",
    "    T_val = pd.Series(0.0, index=antecedents, dtype=float)  # T_{k,Y}\n",
    "    D_val = pd.Series(np.nan, index=antecedents, dtype=float)  # D_{k,Y}\n",
    "    lambda_l: Dict[str, pd.Series] = {}  # per-k vector of λ_{k,j} for j after k and before Y\n",
    "\n",
    "    for k in antecedents:\n",
    "        # ---- Contingency for (k, outcome) ----\n",
    "        row_ko = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == outcome)]\n",
    "\n",
    "        # a: Y∩k    b: ¬Y∩k    c: Y∩¬k    d: ¬Y∩¬k\n",
    "        a = float(row_ko[\"n_code_target\"].sum(skipna=True))            # co-occurrence in Y\n",
    "        b = float(row_ko[\"n_code_no_target\"].sum(skipna=True))         # co-occurrence in ¬Y\n",
    "\n",
    "        # If n_target_no_code absent, approximate c by (max n_target - a)\n",
    "        if \"n_target_no_code\" in row_ko.columns:\n",
    "            c = float(row_ko[\"n_target_no_code\"].sum(skipna=True))\n",
    "        else:\n",
    "            c = float(pd.to_numeric(row_ko[\"n_target\"], errors=\"coerce\").max() - a)\n",
    "\n",
    "        # If no_code_no_target absent, approximate d by (max n_no_target - b)\n",
    "        if \"no_code_no_target\" in row_ko.columns:\n",
    "            d = float(row_ko[\"no_code_no_target\"].sum(skipna=True))\n",
    "        else:\n",
    "            d = float(pd.to_numeric(row_ko[\"n_no_target\"], errors=\"coerce\").max() - b)\n",
    "\n",
    "        N1, N0 = a + b, c + d  # stratum sizes for Z_k=1 and Z_k=0\n",
    "\n",
    "        # ---- Sample-size–anchored odds for Z_k=1 ----\n",
    "        if a == 0:\n",
    "            odds_pos_adj = 1.0 / (N1 + 1.0)  # 0 → tiny positive odds\n",
    "        elif b == 0:\n",
    "            odds_pos_adj = (N1 + 1.0) / 1.0  # ∞ → capped by (N1+1)\n",
    "        else:\n",
    "            odds_pos_adj = a / b\n",
    "\n",
    "        # ---- Sample-size–anchored odds for Z_k=0 ----\n",
    "        if c == 0:\n",
    "            odds_neg_adj = 1.0 / (N0 + 1.0)\n",
    "        elif d == 0:\n",
    "            odds_neg_adj = (N0 + 1.0) / 1.0\n",
    "        else:\n",
    "            odds_neg_adj = c / d\n",
    "\n",
    "        # Total effect T_{k,Y}\n",
    "        T_val.loc[k] = float(odds_pos_adj / odds_neg_adj)\n",
    "\n",
    "        # ---- λ_{k,j}: dependence of j on k for nodes j after k (and before Y) ----\n",
    "        pos_k = events_order.index(k)\n",
    "        js = events_order[pos_k + 1 : -1] if pos_k < len(events_order) - 1 else []\n",
    "\n",
    "        lam_pairs = []\n",
    "        for j in js:\n",
    "            row_kj = df[(df[\"target_concept_code\"] == k) & (df[\"concept_code\"] == j)]\n",
    "            if row_kj.empty:\n",
    "                lam_pairs.append((j, 0.0))  # no evidence of dependence\n",
    "                continue\n",
    "\n",
    "            # Prefer precomputed total_effect if present for this edge\n",
    "            #te = float(pd.to_numeric(row_kj[\"total_effect\"], errors=\"coerce\").max()) if has_total else float(\"nan\")\n",
    "            #if np.isfinite(te):\n",
    "            #    lam_pairs.append((j, te))\n",
    "            #    continue\n",
    "\n",
    "            # Otherwise approximate λ with piecewise, size-anchored logic\n",
    "            # C11 = C(j∩k); Cj_not_k = C(j∩¬k); Ck = C(k)\n",
    "            C11 = float(row_kj[\"n_code_target\"].sum(skipna=True))  # re-using the same column name for co-occurrence\n",
    "            if \"n_code_no_target\" in row_kj.columns:\n",
    "                Cj_not_k = float(row_kj[\"n_code_no_target\"].sum(skipna=True))\n",
    "            else:\n",
    "                Cj = C_of(j)\n",
    "                Cj_not_k = 0.0 if (not np.isfinite(Cj)) else max(Cj - C11, 0.0)\n",
    "            Ck = C_of(k)\n",
    "\n",
    "            if Cj_not_k == 0:\n",
    "                L = 1.0 + C11          # always-with-k → boost\n",
    "            elif C11 == 0:\n",
    "                L = 1.0 / (1.0 + Cj_not_k)  # never-with-k → downweight\n",
    "            elif np.isfinite(Ck) and Ck > 0:\n",
    "                L = C11 / Ck           # normalized co-occurrence\n",
    "            else:\n",
    "                L = 0.0\n",
    "\n",
    "            lam_pairs.append((j, float(L)))\n",
    "\n",
    "        lambda_l[k] = pd.Series({j: v for j, v in lam_pairs}, dtype=float)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4) BACKWARD RECURSION — resolve D_{k,Y} from T and λ\n",
    "    # ---------------------------------------------------------------------\n",
    "    # D_{k,Y} = ( T_{k,Y} - Σ_i λ_{k,k+i} * D_{k+i,Y} ) / ( 1 - Σ_i λ_{k,k+i} )\n",
    "    # Start at the last antecedent (just before Y): D := T, since there are no\n",
    "    # downstream nodes to adjust for.\n",
    "\n",
    "    trace_rows = []  # for human-auditable tracing of the recursion\n",
    "\n",
    "    last_anc = antecedents[-1] if antecedents else None\n",
    "    if last_anc is not None:\n",
    "        D_val.loc[last_anc] = T_val.loc[last_anc]\n",
    "        trace_rows.append({\n",
    "            \"stage\": \"Last 2 Nodes\",\n",
    "            \"nodes\": f\"{last_anc} - {outcome}\",\n",
    "            \"k\": last_anc,\n",
    "            \"T_kY\": T_val.loc[last_anc],\n",
    "            \"lambda_terms\": None,\n",
    "            \"sum_lambda\": 0.0,\n",
    "            \"D_kY\": D_val.loc[last_anc],\n",
    "            \"log_D\": safe_log(D_val.loc[last_anc]),\n",
    "        })\n",
    "\n",
    "    if len(antecedents) > 1:\n",
    "        # Walk backwards through the remaining antecedents\n",
    "        for k in list(reversed(antecedents))[1:]:\n",
    "            lam_vec = lambda_l.get(k, pd.Series(dtype=float))\n",
    "            # When computing the adjustment, use **only** D-values for nodes that\n",
    "            # are after k and already resolved (present in lam_vec index).\n",
    "            code = list(lam_vec.index)\n",
    "            num = T_val.loc[k] - float(np.nansum(lam_vec.reindex(code).values * D_val.reindex(code).values))\n",
    "            den = 1.0 - float(np.nansum(lam_vec.values))  # may approach 0 if λ's are large\n",
    "\n",
    "            # If den is pathological (≤0 or NaN), fall back to T (neutralization).\n",
    "            D_val.loc[k] = (num / den) if np.isfinite(num / den) else T_val.loc[k]\n",
    "\n",
    "            span = len(events_order) - events_order.index(k) + 1\n",
    "            lam_str = \", \".join(\n",
    "                f\"λ_{events_order.index(k)+1}{events_order.index(c)+1}={lam_vec[c]:.6f}\"\n",
    "                for c in code\n",
    "            ) if len(lam_vec) else None\n",
    "\n",
    "            trace_rows.append({\n",
    "                \"stage\": f\"Last {span} Nodes\",\n",
    "                \"nodes\": \" - \".join([k] + events_order[events_order.index(k)+1:]),\n",
    "                \"k\": k,\n",
    "                \"T_kY\": T_val.loc[k],\n",
    "                \"lambda_terms\": lam_str,\n",
    "                \"sum_lambda\": float(np.nansum(lam_vec.values)),\n",
    "                \"D_kY\": D_val.loc[k],\n",
    "                \"log_D\": safe_log(D_val.loc[k]),\n",
    "            })\n",
    "\n",
    "    trace_df = pd.DataFrame(trace_rows)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 5) COEFFICIENTS — map D's onto a logistic link\n",
    "    # ---------------------------------------------------------------------\n",
    "    # We model:  logit P(Y=1 | Z) = β0 + Σ_k β_k * Z_k,  with β_k = log D_{k,Y}\n",
    "    # Intercept β0 is set by the marginal prevalence of Y (from `n_target` &\n",
    "    # `n_no_target`) for the outcome rows.\n",
    "\n",
    "    resp_rows = df[df[\"target_concept_code\"] == outcome]\n",
    "    if resp_rows.empty:\n",
    "        raise ValueError(f\"No rows for outcome '{outcome}'.\")\n",
    "\n",
    "    n_t = resp_rows[\"n_target\"].dropna().iloc[0] if resp_rows[\"n_target\"].dropna().size else np.nan\n",
    "    n_n = resp_rows[\"n_no_target\"].dropna().iloc[0] if resp_rows[\"n_no_target\"].dropna().size else np.nan\n",
    "    denom = n_t + n_n\n",
    "    p_y = 0.5 if (not np.isfinite(denom) or denom <= 0) else (n_t / denom)\n",
    "    beta_0 = float(np.log(p_y / (1 - p_y)))  # stable enough for p∈(0,1)\n",
    "\n",
    "    # β_k = log(D_{k,Y}); protect against non-positive D by mapping to 0\n",
    "    D_clean = pd.to_numeric(D_val, errors=\"coerce\").astype(float)\n",
    "    D_pos = D_clean.where(D_clean > 0)\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        beta_vals = np.log(D_pos.to_numpy())\n",
    "    beta_k_raw = pd.Series(beta_vals, index=D_val.index)\n",
    "    invalid_predictors = list(beta_k_raw[~np.isfinite(beta_k_raw)].index)\n",
    "\n",
    "    beta_k = beta_k_raw.copy()\n",
    "    beta_k[~np.isfinite(beta_k)] = 0.0  # neutralize invalid predictors\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"predictor\": list(beta_k.index) + [\"(intercept)\"],\n",
    "        \"beta\": list(beta_k.astype(float).values) + [beta_0],\n",
    "    })\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 6) PREDICT_PROBA — vectorized logistic link\n",
    "    # ---------------------------------------------------------------------\n",
    "    predictors = list(beta_k.index)\n",
    "    beta_vec = beta_k.astype(float).values\n",
    "\n",
    "    def predict_proba(z: Union[Dict[str, Any], pd.Series, np.ndarray, List[float], pd.DataFrame]) -> Union[float, np.ndarray, pd.Series]:\n",
    "        \"\"\"Compute P(Y=1 | Z) using the logistic link.\n",
    "\n",
    "        Accepts:\n",
    "          - dict/Series mapping predictor name -> 0/1\n",
    "          - 1D/2D numpy/list with columns ordered as `predictors`\n",
    "          - DataFrame with columns containing any/all of `predictors` (others ignored)\n",
    "\n",
    "        Returns:\n",
    "          - float for 1D inputs; np.ndarray or pd.Series for vectorized inputs\n",
    "        \"\"\"\n",
    "        if isinstance(z, pd.DataFrame):\n",
    "            Z = z.reindex(columns=predictors, fill_value=0).astype(float).to_numpy()\n",
    "            eta = beta_0 + Z @ beta_vec\n",
    "            # Stable sigmoid via clipping; avoids overflow for extreme η\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "        if isinstance(z, (dict, pd.Series)):\n",
    "            v = np.array([float(z.get(p, 0.0)) for p in predictors], dtype=float)\n",
    "            eta = beta_0 + float(v @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "\n",
    "        arr = np.asarray(z, dtype=float)\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != len(predictors):\n",
    "                raise ValueError(f\"Expected {len(predictors)} features in order: {predictors}\")\n",
    "            eta = beta_0 + float(arr @ beta_vec)\n",
    "            return float(1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700))))\n",
    "        else:\n",
    "            if arr.shape[1] != len(predictors):\n",
    "                raise ValueError(f\"Expected shape (*, {len(predictors)}), got {arr.shape}\")\n",
    "            eta = beta_0 + arr @ beta_vec\n",
    "            return 1.0 / (1.0 + np.exp(-np.clip(eta, -700, 700)))\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7) PACKAGE RESULTS\n",
    "    # ---------------------------------------------------------------------\n",
    "    return {\n",
    "        \"sorted_scores\": sorted_scores,\n",
    "        \"temporal_order\": temporal_order,\n",
    "        \"order_used\": events_order,\n",
    "        \"T_val\": T_val,\n",
    "        \"D_val\": D_val,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"lambda_l\": lambda_l,\n",
    "        \"trace_df\": trace_df,\n",
    "        \"invalid_predictors\": invalid_predictors,\n",
    "        # Logistic link outputs:\n",
    "        \"beta_0\": beta_0,\n",
    "        \"beta\": pd.Series(beta_vec, index=predictors, dtype=float),\n",
    "        \"logit_predictors\": predictors,\n",
    "        \"predict_proba\": predict_proba,\n",
    "    }\n",
    "\n",
    "# ========= helpers =========\n",
    "\n",
    "def _ensure_derived_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure derived counts (n_target_no_code, no_code_no_target) exist,\n",
    "    coerce numeric columns safely, and (if absent) compute per-edge total_effect\n",
    "    using sample-size–anchored odds. Returns a copy.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # ---- 1) Base numeric columns ----\n",
    "    base = [\"n_code_target\", \"n_code_no_target\", \"n_target\", \"n_no_target\"]\n",
    "    for c in base:\n",
    "        if c not in out.columns:\n",
    "            out[c] = 0\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).clip(lower=0).astype(float)\n",
    "\n",
    "    # ---- 2) Complements (clip to ≥ 0) ----\n",
    "    # Y & (k=0)\n",
    "    if \"n_target_no_code\" not in out.columns:\n",
    "        out[\"n_target_no_code\"] = (out[\"n_target\"] - out[\"n_code_target\"]).clip(lower=0)\n",
    "    else:\n",
    "        out[\"n_target_no_code\"] = pd.to_numeric(out[\"n_target_no_code\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "\n",
    "    # (k=0) & (Y=0)\n",
    "    if \"no_code_no_target\" not in out.columns:\n",
    "        out[\"no_code_no_target\"] = (out[\"n_no_target\"] - out[\"n_code_no_target\"]).clip(lower=0)\n",
    "    else:\n",
    "        out[\"no_code_no_target\"] = pd.to_numeric(out[\"no_code_no_target\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "\n",
    "    # ---- 3) total_effect (odds_pos / odds_neg) if absent ----\n",
    "    if \"total_effect\" not in out.columns:\n",
    "        a  = out[\"n_code_target\"].astype(float)        # k=1, Y=1\n",
    "        b  = out[\"n_code_no_target\"].astype(float)     # k=1, Y=0\n",
    "        c  = out[\"n_target_no_code\"].astype(float)     # k=0, Y=1\n",
    "        d  = out[\"no_code_no_target\"].astype(float)    # k=0, Y=0\n",
    "\n",
    "        N1 = a + b\n",
    "        N0 = c + d\n",
    "\n",
    "        # Sample-size–anchored odds with neutral fallback when stratum size is zero\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            odds_pos = np.where(\n",
    "                N1 == 0, 1.0,                              # no data in Z_k=1 → neutral\n",
    "                np.where(\n",
    "                    (a > 0) & (b > 0), a / b,\n",
    "                    np.where(b == 0, N1 + 1.0, 1.0 / (N1 + 1.0))\n",
    "                )\n",
    "            )\n",
    "            odds_neg = np.where(\n",
    "                N0 == 0, 1.0,                              # no data in Z_k=0 → neutral\n",
    "                np.where(\n",
    "                    (c > 0) & (d > 0), c / d,\n",
    "                    np.where(d == 0, N0 + 1.0, 1.0 / (N0 + 1.0))\n",
    "                )\n",
    "            )\n",
    "\n",
    "            te = odds_pos / odds_neg\n",
    "\n",
    "        # Clean any residual non-finite values to neutral 1.0\n",
    "        te = np.where(np.isfinite(te), te, 1.0)\n",
    "        out[\"total_effect\"] = te.astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "# TOPK_TE\n",
    "def _select_top_by_te_unique_k(k_to_T: pd.DataFrame, top_k: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select predictors strictly by total_effect (descending), keeping ONE row per predictor k.\n",
    "    Predictor k is the LEFT side in the row → column 'target_concept_code'.\n",
    "    \"\"\"\n",
    "    df = k_to_T.copy()\n",
    "    df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effect\"])\n",
    "    if df.empty:\n",
    "        print(\"[SELECT] no rows with total_effect; returning empty selection.\")\n",
    "        return df\n",
    "\n",
    "    # Sort by TE desc, then tie-break deterministically by k code\n",
    "    sort_cols = [\"total_effect\", \"target_concept_code\"]\n",
    "    df = df.sort_values(sort_cols, ascending=[False, True])\n",
    "\n",
    "    # Keep most-extreme row per k (target_concept_code)\n",
    "    best_per_k = df.drop_duplicates(subset=[\"target_concept_code\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    take_n = min(int(top_k), len(best_per_k))\n",
    "    selected = best_per_k.head(take_n).copy()\n",
    "    selected.insert(0, \"rank_by_TE\", np.arange(1, len(selected)+1))\n",
    "    return selected\n",
    "\n",
    "# TOPK_HI/LO\n",
    "def _select_top_by_te_unique_k(k_to_T: pd.DataFrame, top_k: int = TOP_K,\n",
    "                                   hi: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balanced selection without fallback:\n",
    "      - Take up to top_k//2 strongest risk (TE >= hi)\n",
    "      - Take up to top_k - top_k//2 strongest protective (TE <= 1/hi)\n",
    "      - If either side is short, do NOT fill from elsewhere; total may be < top_k\n",
    "      - Return one row per k (target_concept_code), ranked by extremeness\n",
    "    \"\"\"\n",
    "    df = k_to_T.copy()\n",
    "\n",
    "    # Ensure total_effect numeric\n",
    "    df[\"total_effect\"] = pd.to_numeric(df[\"total_effect\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"total_effect\"]).copy()\n",
    "    if df.empty:\n",
    "        print(\"[SELECT] no rows with total_effect; returning empty selection.\")\n",
    "        return df\n",
    "\n",
    "    # Extremeness symmetrical around 1\n",
    "    df[\"effect_strength\"] = np.where(df[\"total_effect\"] >= 1.0,\n",
    "                                     df[\"total_effect\"],\n",
    "                                     1.0 / df[\"total_effect\"])\n",
    "\n",
    "    # One row per k (keep most extreme)\n",
    "    best_per_k = (df.sort_values(\"effect_strength\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"target_concept_code\"], keep=\"first\"))\n",
    "\n",
    "    HI = hi\n",
    "    LO = 1.0 / hi\n",
    "\n",
    "    risk_pool = best_per_k[best_per_k[\"total_effect\"] >= HI].copy()\n",
    "    prot_pool = best_per_k[best_per_k[\"total_effect\"] <= LO].copy()\n",
    "\n",
    "    # Target split\n",
    "    want_risk = top_k // 2\n",
    "    want_prot = top_k - want_risk\n",
    "\n",
    "    # Take strongest from each side, capped by availability\n",
    "    take_risk = min(len(risk_pool), want_risk)\n",
    "    take_prot = min(len(prot_pool), want_prot)\n",
    "\n",
    "    sel_risk = risk_pool.nlargest(take_risk, \"effect_strength\")\n",
    "    sel_prot = prot_pool.nlargest(take_prot, \"effect_strength\")\n",
    "\n",
    "    selected = pd.concat([sel_risk, sel_prot], ignore_index=True)\n",
    "\n",
    "    print(f\"[SELECT] total unique k={len(best_per_k):,}  \"\n",
    "          f\"risk={len(risk_pool):,}  prot={len(prot_pool):,}  \"\n",
    "          f\"selected(total)={selected['target_concept_code'].nunique()}  \"\n",
    "          f\"with: risk={take_risk}, prot={take_prot}\")\n",
    "\n",
    "    return selected.reset_index(drop=True)\n",
    "def _fetch_k_to_T(conn, target_code: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch only k→T rows (concept_code == target).\"\"\"\n",
    "    q = \"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- k (predictor)\n",
    "             ccn.concept_code AS concept_code           -- T (outcome)\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE ccn.concept_code = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[target_code])\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list):\n",
    "    \"\"\"Fetch edges with target_concept_code IN events_set (single IN to avoid 999 param issues).\"\"\"\n",
    "    ph = \",\".join([\"?\"] * len(events_list))\n",
    "    q = f\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,\n",
    "             ccn.concept_code AS concept_code\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE tcn.concept_code IN ({ph})\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=list(events_list))\n",
    "def _fetch_k_to_T(conn, target_code: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch only k→T rows (concept_code == target).\"\"\"\n",
    "    q = \"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,   -- k (predictor)\n",
    "             ccn.concept_code AS concept_code           -- T (outcome)\n",
    "      FROM magi_counts_top500 m\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      WHERE ccn.concept_code = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(q, conn, params=[target_code])\n",
    "\n",
    "def _fetch_subgraph_by_targets(conn, events_list, counts_table=\"magi_counts_top500\"):\n",
    "    \"\"\"\n",
    "    Return rows whose BOTH ends are in events_list by joining against a TEMP table,\n",
    "    avoiding SQLite's 'too many SQL variables' limit.\n",
    "    \"\"\"\n",
    "    # normalize/guard\n",
    "    ev = sorted({str(x) for x in events_list if x is not None})\n",
    "    if not ev:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # create temp table of events\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"DROP TABLE IF EXISTS tmp_events\")\n",
    "    cur.execute(\"CREATE TEMP TABLE tmp_events (concept_code TEXT PRIMARY KEY)\")\n",
    "    cur.executemany(\"INSERT OR IGNORE INTO tmp_events(concept_code) VALUES (?)\",\n",
    "                    [(x,) for x in ev])\n",
    "    conn.commit()  # ensure visibility to read_sql_query\n",
    "\n",
    "    q = f\"\"\"\n",
    "        SELECT\n",
    "            m.*,\n",
    "            tcn.concept_code AS target_concept_code,\n",
    "            ccn.concept_code AS concept_code\n",
    "        FROM {counts_table} AS m\n",
    "        JOIN concept_names AS tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "        JOIN concept_names AS ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "        JOIN tmp_events AS te1     ON tcn.concept_code = te1.concept_code   -- target end in set\n",
    "        JOIN tmp_events AS te2     ON ccn.concept_code = te2.concept_code   -- concept end in set\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(q, conn)  # no giant params list needed\n",
    "    # optional cleanup\n",
    "    cur.execute(\"DROP TABLE IF EXISTS tmp_events\")\n",
    "    return df\n",
    "\n",
    "## Remove dup\n",
    "# ========= main loop =========\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    for T in TARGETS:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "        # 1) k→T rows (concept_code == T)\n",
    "        k_to_T = _fetch_k_to_T(conn, T)\n",
    "\n",
    "        # --- NEW: drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(k_to_T)\n",
    "        k_to_T = k_to_T.drop_duplicates()\n",
    "        after = len(k_to_T)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] k_to_T exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        if k_to_T.empty:\n",
    "            print(f\"[WARN] No predictor→target rows for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2) derive totals/effects on the k→T list\n",
    "        k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "        # 3) select predictors: top-K by total_effect (one row per k)\n",
    "        sel_rows = _select_top_by_te_unique_k(k_to_T, top_k=TOP_K)\n",
    "        if sel_rows.empty:\n",
    "            print(f\"[WARN] No predictors selected for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"risk_prot_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Factors → {sub_csv}\")\n",
    "\n",
    "        # IMPORTANT: predictors are the LEFT side (target_concept_code)\n",
    "        selected_k = set(sel_rows[\"target_concept_code\"].astype(str))\n",
    "        print(f\"[SELECT] unique k available={k_to_T['target_concept_code'].nunique():,}  \"\n",
    "              f\"selected={len(selected_k):,}\")\n",
    "\n",
    "        # 4) build subgraph among {T} ∪ selected_k\n",
    "        events_set = selected_k | {T}\n",
    "        df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "        df_trim = df_trim[\n",
    "            df_trim[\"target_concept_code\"].isin(events_set) &\n",
    "            df_trim[\"concept_code\"].isin(events_set)\n",
    "        ].copy()\n",
    "\n",
    "        # --- NEW: drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(df_trim)\n",
    "        df_trim = df_trim.drop_duplicates()\n",
    "        after = len(df_trim)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        # Recompute derived cols (incl. total_effect) on the subgraph\n",
    "        df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "        # Correct orientation in the prints:\n",
    "        #   k→T rows: concept_code == T\n",
    "        #   T→j rows: target_concept_code == T\n",
    "        print(f\"[TRIM] rows={len(df_trim):,}  events={len(events_set)}  \"\n",
    "              f\"k→T rows={(df_trim['concept_code'] == T).sum()}  \"\n",
    "              f\"T→j rows={(df_trim['target_concept_code'] == T).sum()}\")\n",
    "\n",
    "        # 5) save subgraph (for audit)\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "        # 6) run MAGI\n",
    "        try:\n",
    "            res = analyze_causal_sequence_py(df_trim, events=None, name_map=None, force_outcome=T)\n",
    "        except TypeError:\n",
    "            res = analyze_causal_sequence_py(df_trim)\n",
    "            outcome_used = res.get(\"order_used\", [None])[-1]\n",
    "            if outcome_used != T:\n",
    "                print(f\"[NOTE] outcome auto-inferred as {outcome_used}, not {T}\")\n",
    "\n",
    "        # 7) save NON-ZERO coefficients only\n",
    "        outcome_used = res.get(\"order_used\", [T])[-1]\n",
    "        coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "        # robust: accept 'coef' or 'beta' column name\n",
    "        coef_col = \"coef\" if \"coef\" in coef_df.columns else (\"beta\" if \"beta\" in coef_df.columns else None)\n",
    "        if coef_col is None:\n",
    "            raise KeyError(\"Coefficient column not found in coef_df (expected 'coef' or 'beta').\")\n",
    "\n",
    "        # filter to non-zero (tolerance to avoid tiny numerical noise)\n",
    "        eps = 1e-12\n",
    "        mask_nz = coef_df[coef_col].astype(float).abs() > eps\n",
    "        coef_nz = coef_df.loc[mask_nz].copy()\n",
    "\n",
    "        coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}_nonzero.csv\")\n",
    "        coef_nz.to_csv(coef_csv, index=False)\n",
    "\n",
    "        print(f\"[SAVED] Coefficients (non-zero only) → {coef_csv}  \"\n",
    "              f\"| kept={len(coef_nz):,} of {len(coef_df):,}  \"\n",
    "              f\"| antecedents={len(res.get('order_used', [])) - 1}  \"\n",
    "              f\"| used_total_effect={res.get('used_total_effect', True)}\")\n",
    "# ========= main loop =========\n",
    "import os, sqlite3\n",
    "\n",
    "# ensure OUT_DIR exists\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    for T in TARGETS:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "        # 1) k→T rows (concept_code == T)\n",
    "        k_to_T = _fetch_k_to_T(conn, T)\n",
    "\n",
    "        # --- NEW: drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(k_to_T)\n",
    "        k_to_T = k_to_T.drop_duplicates()\n",
    "        after = len(k_to_T)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] k_to_T exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        if k_to_T.empty:\n",
    "            print(f\"[WARN] No predictor→target rows for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2) derive totals/effects on the k→T list\n",
    "        k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "        # 3) select predictors: top-K by total_effect (one row per k)\n",
    "        sel_rows = _select_top_by_te_unique_k(k_to_T, top_k=TOP_K)\n",
    "        if sel_rows.empty:\n",
    "            print(f\"[WARN] No predictors selected for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # save the selected k→T factor list (NOT df_trim; it doesn't exist yet)\n",
    "        factors_csv = os.path.join(OUT_DIR, f\"risk_prot_{T}.csv\")\n",
    "        sel_rows.to_csv(factors_csv, index=False)\n",
    "        print(f\"[SAVED] Factors → {factors_csv}\")\n",
    "\n",
    "        # IMPORTANT: predictors are the LEFT side (target_concept_code)\n",
    "        selected_k = set(sel_rows[\"target_concept_code\"].astype(str))\n",
    "        print(\n",
    "            f\"[SELECT] unique k available={k_to_T['target_concept_code'].nunique():,}  \"\n",
    "            f\"selected={len(selected_k):,}\"\n",
    "        )\n",
    "\n",
    "        # 4) build subgraph among {T} ∪ selected_k\n",
    "        events_set = selected_k | {str(T)}\n",
    "        df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "        # keep only rows fully inside the event set\n",
    "        df_trim = df_trim[\n",
    "            df_trim[\"target_concept_code\"].astype(str).isin(events_set) &\n",
    "            df_trim[\"concept_code\"].astype(str).isin(events_set)\n",
    "        ].copy()\n",
    "\n",
    "        # --- NEW: drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(df_trim)\n",
    "        df_trim = df_trim.drop_duplicates()\n",
    "        after = len(df_trim)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        # Recompute derived cols (incl. total_effect) on the subgraph\n",
    "        df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "        # Correct orientation in the prints:\n",
    "        #   k→T rows: concept_code == T\n",
    "        #   T→j rows: target_concept_code == T\n",
    "        n_k_to_T = int((df_trim[\"concept_code\"].astype(str) == str(T)).sum())\n",
    "        n_T_to_j = int((df_trim[\"target_concept_code\"].astype(str) == str(T)).sum())\n",
    "        print(\n",
    "            f\"[TRIM] rows={len(df_trim):,}  events={len(events_set)}  \"\n",
    "            f\"k→T rows={n_k_to_T}  T→j rows={n_T_to_j}\"\n",
    "        )\n",
    "\n",
    "        # 5) save subgraph (for audit)\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "        # 6) run MAGI\n",
    "        try:\n",
    "            res = analyze_causal_sequence_py(df_trim, events=None, name_map=None, force_outcome=str(T))\n",
    "        except TypeError:\n",
    "            # fallback for older signature\n",
    "            res = analyze_causal_sequence_py(df_trim)\n",
    "            outcome_used_fallback = res.get(\"order_used\", [None])[-1]\n",
    "            if outcome_used_fallback != str(T):\n",
    "                print(f\"[NOTE] outcome auto-inferred as {outcome_used_fallback}, not {T}\")\n",
    "\n",
    "        if not isinstance(res, dict) or \"coef_df\" not in res:\n",
    "            raise RuntimeError(\"analyze_causal_sequence_py did not return expected result (missing 'coef_df').\")\n",
    "\n",
    "        # 7) save NON-ZERO coefficients only\n",
    "        outcome_used = res.get(\"order_used\", [str(T)])[-1]\n",
    "        coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "        # robust: accept 'coef' or 'beta' column name\n",
    "        coef_col = \"coef\" if \"coef\" in coef_df.columns else (\"beta\" if \"beta\" in coef_df.columns else None)\n",
    "        if coef_col is None:\n",
    "            raise KeyError(\"Coefficient column not found in coef_df (expected 'coef' or 'beta').\")\n",
    "\n",
    "        # filter to non-zero (tolerance to avoid tiny numerical noise)\n",
    "        eps = 1e-12\n",
    "        mask_nz = coef_df[coef_col].astype(float).abs() > eps\n",
    "        coef_nz = coef_df.loc[mask_nz].copy()\n",
    "\n",
    "        coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}_nonzero.csv\")\n",
    "        coef_nz.to_csv(coef_csv, index=False)\n",
    "\n",
    "        print(\n",
    "            f\"[SAVED] Coefficients (non-zero only) → {coef_csv}  \"\n",
    "            f\"| kept={len(coef_nz):,} of {len(coef_df):,}  \"\n",
    "            f\"| antecedents={max(0, len(res.get('order_used', [])) - 1)}  \"\n",
    "            f\"| used_total_effect={res.get('used_total_effect', True)}\"\n",
    "        )\n",
    "\n",
    "# ========= main loop (remove duplicates; keep ALL predictors) =========\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    for T in TARGETS:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "        # 1) k→T rows (concept_code == T)\n",
    "        k_to_T = _fetch_k_to_T(conn, T)\n",
    "\n",
    "        # --- drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(k_to_T)\n",
    "        k_to_T = k_to_T.drop_duplicates()\n",
    "        after = len(k_to_T)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] k_to_T exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        if k_to_T.empty:\n",
    "            print(f\"[WARN] No predictor→target rows for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2) derive totals/effects on the k→T list\n",
    "        k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "        # 3) KEEP ALL predictors (no TOP-K filtering)\n",
    "        #    'selected_k' is the full unique set of antecedent codes on the LEFT side (target_concept_code)\n",
    "        selected_k = set(k_to_T[\"target_concept_code\"].astype(str).unique())\n",
    "        print(f\"[SELECT] unique k available={len(selected_k):,}  (keeping ALL)\")\n",
    "\n",
    "        # Save the full k→T table for audit/import\n",
    "        vars_csv = os.path.join(OUT_DIR, f\"magi_vars_{T}_ALL.csv\")\n",
    "        k_to_T.to_csv(vars_csv, index=False)\n",
    "        print(f\"[SAVED] Variables (k→T ALL) → {vars_csv}\")\n",
    "\n",
    "        # 4) build subgraph among {T} ∪ selected_k\n",
    "        events_set = selected_k | {T}\n",
    "        df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "        df_trim = df_trim[\n",
    "            df_trim[\"target_concept_code\"].isin(events_set) &\n",
    "            df_trim[\"concept_code\"].isin(events_set)\n",
    "        ].copy()\n",
    "\n",
    "        # --- drop exact duplicate rows (all columns identical) ---\n",
    "        before = len(df_trim)\n",
    "        df_trim = df_trim.drop_duplicates()\n",
    "        after = len(df_trim)\n",
    "        if after < before:\n",
    "            print(f\"[DEDUP] subgraph exact-row duplicates removed: {before - after}  (kept {after})\")\n",
    "\n",
    "        # Recompute derived cols (incl. total_effect) on the subgraph\n",
    "        df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "        # Correct orientation in the prints:\n",
    "        print(f\"[TRIM] rows={len(df_trim):,}  events={len(events_set)}  \"\n",
    "              f\"k→T rows={(df_trim['concept_code'] == T).sum()}  \"\n",
    "              f\"T→j rows={(df_trim['target_concept_code'] == T).sum()}\")\n",
    "\n",
    "        # 5) save subgraph (for audit)\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "        # 6) run MAGI\n",
    "        try:\n",
    "            res = analyze_causal_sequence_py(df_trim, events=None, name_map=None, force_outcome=T)\n",
    "        except TypeError:\n",
    "            res = analyze_causal_sequence_py(df_trim)\n",
    "            outcome_used = res.get(\"order_used\", [None])[-1]\n",
    "            if outcome_used != T:\n",
    "                print(f\"[NOTE] outcome auto-inferred as {outcome_used}, not {T}\")\n",
    "\n",
    "        # 7) save NON-ZERO coefficients only\n",
    "        outcome_used = res.get(\"order_used\", [T])[-1]\n",
    "        coef_df = res[\"coef_df\"].copy()\n",
    "\n",
    "        # robust: accept 'coef' or 'beta' column name\n",
    "        coef_col = \"coef\" if \"coef\" in coef_df.columns else (\"beta\" if \"beta\" in coef_df.columns else None)\n",
    "        if coef_col is None:\n",
    "            raise KeyError(\"Coefficient column not found in coef_df (expected 'coef' or 'beta').\")\n",
    "\n",
    "        # filter to non-zero (tolerance to avoid tiny numerical noise)\n",
    "        eps = 1e-12\n",
    "        mask_nz = coef_df[coef_col].astype(float).abs() > eps\n",
    "        coef_nz = coef_df.loc[mask_nz].copy()\n",
    "\n",
    "        coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}_nonzero.csv\")\n",
    "        coef_nz.to_csv(coef_csv, index=False)\n",
    "\n",
    "        print(f\"[SAVED] Coefficients (non-zero only) → {coef_csv}  \"\n",
    "              f\"| kept={len(coef_nz):,} of {len(coef_df):,}  \"\n",
    "              f\"| antecedents={len(res.get('order_used', [])) - 1}  \"\n",
    "              f\"| used_total_effect={res.get('used_total_effect', True)}\")\n",
    "\n",
    "# ========= main loop =========\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    for T in TARGETS:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"[RUN] Target = {T}\")\n",
    "\n",
    "        # 1) k→T rows (concept_code == T)\n",
    "        k_to_T = _fetch_k_to_T(conn, T)\n",
    "        if k_to_T.empty:\n",
    "            print(f\"[WARN] No predictor→target rows for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2) derive totals/effects on the k→T list\n",
    "        k_to_T = _ensure_derived_cols(k_to_T)\n",
    "\n",
    "        # 3) select predictors: top-K by total_effect (one row per k)\n",
    "        sel_rows = _select_top_by_te_unique_k(k_to_T, top_k=TOP_K)\n",
    "        if sel_rows.empty:\n",
    "            print(f\"[WARN] No predictors selected for {T}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # IMPORTANT: predictors are the LEFT side (target_concept_code)\n",
    "        selected_k = set(sel_rows[\"target_concept_code\"].astype(str))\n",
    "        print(f\"[SELECT] unique k available={k_to_T['target_concept_code'].nunique():,}  \"\n",
    "              f\"selected={len(selected_k):,}\")\n",
    "\n",
    "        # Save the selected variables list for audit/import\n",
    "        vars_csv = os.path.join(OUT_DIR, f\"magi_vars_{T}_topByTE.csv\")\n",
    "        sel_rows.to_csv(vars_csv, index=False)\n",
    "        print(f\"[SAVED] Variables (k→T top-by-TE) → {vars_csv}\")\n",
    "\n",
    "        # 4) build subgraph among {T} ∪ selected_k\n",
    "        events_set = selected_k | {T}\n",
    "        df_trim = _fetch_subgraph_by_targets(conn, sorted(events_set))\n",
    "        df_trim = df_trim[\n",
    "            df_trim[\"target_concept_code\"].isin(events_set) &\n",
    "            df_trim[\"concept_code\"].isin(events_set)\n",
    "        ].copy()\n",
    "\n",
    "        # Recompute derived cols (incl. total_effect) on the subgraph\n",
    "        df_trim = _ensure_derived_cols(df_trim)\n",
    "\n",
    "        # Correct orientation in the prints:\n",
    "        #   k→T rows: concept_code == T\n",
    "        #   T→j rows: target_concept_code == T\n",
    "        print(f\"[TRIM] rows={len(df_trim):,}  events={len(events_set)}  \"\n",
    "              f\"k→T rows={(df_trim['concept_code'] == T).sum()}  \"\n",
    "              f\"T→j rows={(df_trim['target_concept_code'] == T).sum()}\")\n",
    "\n",
    "        # 5) save subgraph (for audit)\n",
    "        sub_csv = os.path.join(OUT_DIR, f\"magi_subgraph_{T}.csv\")\n",
    "        df_trim.to_csv(sub_csv, index=False)\n",
    "        print(f\"[SAVED] Subgraph → {sub_csv}\")\n",
    "\n",
    "        # 6) run MAGI\n",
    "        try:\n",
    "            res = analyze_causal_sequence_py(df_trim, events=None, name_map=None, force_outcome=T)\n",
    "        except TypeError:\n",
    "            res = analyze_causal_sequence_py(df_trim)\n",
    "            outcome_used = res.get(\"order_used\", [None])[-1]\n",
    "            if outcome_used != T:\n",
    "                print(f\"[NOTE] outcome auto-inferred as {outcome_used}, not {T}\")\n",
    "\n",
    "        # 7) save coefficients\n",
    "        outcome_used = res.get(\"order_used\", [T])[-1]\n",
    "        coef_df = res[\"coef_df\"]\n",
    "        coef_csv = os.path.join(OUT_DIR, f\"magi_coef_{outcome_used}.csv\")\n",
    "        coef_df.to_csv(coef_csv, index=False)\n",
    "        print(f\"[SAVED] Coefficients → {coef_csv}  | antecedents={len(res.get('order_used', [])) - 1}  \"\n",
    "              f\"used_total_effect={res.get('used_total_effect', True)}\")\n",
    "\n",
    "\n",
    "## list all tables\n",
    "\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"  # read-only; no writes\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    # 1) List tables and views\n",
    "    tables = pd.read_sql_query(\"\"\"\n",
    "        SELECT name, type\n",
    "        FROM sqlite_master\n",
    "        WHERE type IN ('table','view')\n",
    "        ORDER BY type, name\n",
    "    \"\"\", conn)\n",
    "    print(\"=== Objects in DB (tables & views) ===\")\n",
    "    print(tables.to_string(index=False))\n",
    "\n",
    "    # 2) Helper to inspect a specific table (schema + sample rows)\n",
    "    def peek(table: str, limit: int = 5):\n",
    "        print(f\"\\n=== Schema: {table} ===\")\n",
    "        schema = pd.read_sql_query(f\"PRAGMA table_info({table})\", conn)\n",
    "        print(schema.to_string(index=False))\n",
    "\n",
    "        print(f\"\\n=== First {limit} rows: {table} ===\")\n",
    "        sample = pd.read_sql_query(f\"SELECT * FROM {table} LIMIT {limit}\", conn)\n",
    "        print(sample)\n",
    "\n",
    "    # Example: uncomment to inspect common tables\n",
    "    # peek(\"concept_names\", 5)\n",
    "    # peek(\"magi_counts_filtered\", 5)\n",
    "    # peek(\"magi_counts_top500\", 5)\n",
    "\n",
    "\n",
    "## shape (rows, columns)\n",
    "def table_shape(conn, table: str):\n",
    "    # columns\n",
    "    cols = pd.read_sql_query(f\"PRAGMA table_info({table})\", conn)\n",
    "    n_cols = len(cols)\n",
    "    # rows (COUNT(*) is the standard way in SQLite)\n",
    "    n_rows = pd.read_sql_query(f\"SELECT COUNT(*) AS n FROM {table}\", conn).loc[0, \"n\"]\n",
    "    return int(n_rows), int(n_cols)\n",
    "\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    for t in [\"magi_counts_filtered\", \"magi_counts_top500\"]:\n",
    "        try:\n",
    "            n_rows, n_cols = table_shape(conn, t)\n",
    "            print(f\"{t}: shape = ({n_rows:,} rows, {n_cols} cols)\")\n",
    "        except Exception as e:\n",
    "            print(f\"{t}: {e}\")\n",
    "\n",
    "\n",
    "uri = f\"file:{MAGI_DB_PATH}?mode=ro\"\n",
    "with sqlite3.connect(uri, uri=True) as conn:\n",
    "    df5 = pd.read_sql_query(\"\"\"\n",
    "      SELECT m.*,\n",
    "             tcn.concept_code AS target_concept_code,\n",
    "             ccn.concept_code AS concept_code\n",
    "      FROM magi_counts_top500 m              -- change to magi_counts / magi_counts_filtered if you want\n",
    "      JOIN concept_names tcn ON m.target_concept_code_int = tcn.concept_code_int\n",
    "      JOIN concept_names ccn ON m.concept_code_int        = ccn.concept_code_int\n",
    "      LIMIT 5\n",
    "    \"\"\", conn)\n",
    "\n",
    "print(df5)         # top 5 rows\n",
    "print(df5.columns) # columns present\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pcoridev)",
   "language": "python",
   "name": "pcoridev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
